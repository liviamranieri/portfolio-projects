{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> IBM Attrition Case </strong> <br>\n",
    "by: Sophie Briques <br>\n",
    "2020-05-29\n",
    "\n",
    "This notebook is complementary to the IBM attrition project that can be found here: https://sbriques.github.io/IBM-Attrition/\n",
    "\n",
    "This was created using Dataiku DSS, and will need adjustments in the data importing to function locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "\n",
    "# Importing dataframe essential packages\n",
    "import dataiku\n",
    "from   dataiku import pandasutils as pdu\n",
    "import pandas as pd\n",
    "\n",
    "# Importing PySpark Packages\n",
    "import dataiku.spark as dkuspark\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, CountVectorizer\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.mllib.evaluation import *\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Load PySpark\n",
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we’re using the dataset cleaned within Dataiku, we run the following lines to read the dataset with dataiku then as Pandas and Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Dataiku dataset from memory\n",
    "mydataset = dataiku.Dataset(\"IBM_clean_Hadoop\")\n",
    "\n",
    "# Read the dataset as a Pandas Dataframe\n",
    "df_pd = mydataset.get_dataframe()\n",
    "\n",
    "# Read the dataset as a Spark dataframe\n",
    "df_sprk = dkuspark.get_dataframe(sqlContext, mydataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll want to check if our dataframe is cleaned. Since we have removed some observations, our dataset should have 1447 rows, and additional features we engineered with SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the count of records in the dataframe\n",
    "print(df_sprk.count())\n",
    "\n",
    "# Get a view of the first 5 records in the dataframe\n",
    "df_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After checking the dataset, we can start building our base model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating vectors with variable names\n",
    "x_var_vec = VectorAssembler(inputCols = ['boolean_overtime',\n",
    "                                         'boolean_businesstravel',\n",
    "                                         'Age',\n",
    "                                         'YearsInCurrentRole',\n",
    "                                         'MonthlyIncome',\n",
    "                                         'StockOptionLevel',\n",
    "                                         'JobSatisfaction',\n",
    "                                         'NumCompaniesWorked',\n",
    "                                         'JobInvolvement'], outputCol = \"features\")\n",
    "\n",
    "# Adding x var vector back into dataframe\n",
    "vec_to_df = x_var_vec.transform(df_sprk)\n",
    "\n",
    "# Defining target variable\n",
    "df_logit = vec_to_df.select(['features', 'boolean_attrition']) \n",
    "\n",
    "# Renaming Target Column\n",
    "df_logit = df_logit.withColumnRenamed(\"boolean_attrition\", \"label\")\n",
    "\n",
    "# Splitting the dataset\n",
    "splits = df_logit.randomSplit([0.7,0.3])\n",
    "train_df = splits[0]\n",
    "test_df = splits[1]\n",
    "\n",
    "# Creating an object with Logistic Regression Model\n",
    "lr       = LogisticRegression(maxIter = 20)\n",
    "pipeline = Pipeline(stages = [lr])\n",
    "\n",
    "# fitting the model \n",
    "model = lr.fit(train_df)\n",
    "\n",
    "# Calculating Evaluation Metrics\n",
    "result    = model.transform(test_df)\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol = \"rawPrediction\")\n",
    "AUC_ROC   = evaluator.evaluate(result,{evaluator.metricName: \"areaUnderROC\"})\n",
    "coefs     = model.coefficients\n",
    "intercept = model.intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll also run a Parameter GridSearch and CrossValidation. These will help improve our model’s predictive capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting up Parameter GridSearch and CrossValidation\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "             .addGrid(lr.maxIter, [1, 5, 10])\n",
    "             .build())\n",
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Run cross validation\n",
    "cvModel = cv.fit(train_df)\n",
    "\n",
    "# Calculating Evaluation Metrics\n",
    "result_cv    = cvModel.transform(test_df)\n",
    "evaluator_cv = BinaryClassificationEvaluator(rawPredictionCol = \"rawPrediction\")\n",
    "AUC_ROC_cv   = evaluator_cv.evaluate(result_cv,{evaluator_cv.metricName: \"areaUnderROC\"})\n",
    "coefs_cv     = cvModel.bestModel.coefficients\n",
    "intercept_cv = cvModel.bestModel.intercept\n",
    "\n",
    "print('LOGISTIC REGRESSION: After CV')\n",
    "print('AUC ROC:' + str(AUC_ROC_cv))\n",
    "print('Coefficients:' +  str(coefs_cv))\n",
    "print('Intercept:' + str(intercept_cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important number in these results is the AUC ROC score (area under the curve). This number represents our model’s predictve capabilities. A score of 0.82 is not bad at all! The coefficients in a logistic regression need to be treated differently than with linear regressions. We first need to take the exponential of a coefficient, which represents the change in odds ratio. For example, for every additional companie worked at in the past, an employees odds of leaving IBM increase by exp(0.015)-1)*100 = 1.56 %."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "creator": "slb-7494576",
  "customFields": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "tags": [],
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
