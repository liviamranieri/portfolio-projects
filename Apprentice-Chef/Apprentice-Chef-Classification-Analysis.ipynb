{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><h1>A2 Report - Classification Modeling Case Study </h1>\n",
    "<h2>Machine Learning - DAT-5303 - FMSBA3 </h2>\n",
    "By: Sophie Briques<br>\n",
    "Hult International Business School<br>\n",
    "Sunday, March 15th, 2019 <br><br><br>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Our best performing model was a Decision Tree with 21 features with a test score of 0.9058\n",
    "- Optimal features were found using exploratory data analysis, domain knowledge and tree classifiers\n",
    "- It is predicting whether or not a customer will subscribe to the new service Halfway There.\n",
    "- Its precision in correctly predicting a new customer is 96%\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Case: Apprentice Chef, Inc. </strong> <br>\n",
    "<i> Audience: Top Executives </i> <br><br>\n",
    "<strong> Context: </strong> <i> Halfway There </i>, new subscription where customers can receive a half bottle of wine from a local California vineyard every Wednesday. <br>\n",
    "<strong> Goal: </strong> when promoting this service to a wider audience, know which customers will subscribe to <i> Halfway There </i> <br>\n",
    "<strong> Target consumer: </strong> busy professional, little to no skills in the kitchen <br>\n",
    "<strong> Product: </strong> daily-prepared gourmet meals delivered <br>\n",
    "<strong> Channels: </strong> online platform and mobile app <br> \n",
    "<strong> Revenue: </strong> 90% of revenue comes from customers that have been ordering for 12 months or less\n",
    "\n",
    "\n",
    "<br> General Product Specifications: \n",
    "- at most 30 min to finish cooking\n",
    "- disposable cookware\n",
    "- delicious and healthy eating<br>\n",
    "\n",
    "<br> Halfway There Specifications:\n",
    "- hard to find local wines\n",
    "- customer need to include government ID in application process to order\n",
    "<br>\n",
    "Channels: online platform and mobile app\n",
    "Revenue: 90% of revenue comes from customers that have been ordering for 12 months or less\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<strong> Data and Assumptions </strong> <br>\n",
    "Dataset:\n",
    "<br> 2,000 customers (approx.)\n",
    "- at least one purchase per month for a total of 11 of their first 12 months\n",
    "- at least one purchase per quarter and at least 15 purchases through their first year\n",
    "- dataset engineering techniques are statistically sound and represent the customers\n",
    "\n",
    "\n",
    "Assumptions:\n",
    "- all average times are in seconds\n",
    "- revenue = price x quantity and total meals ordered represent quantity\n",
    "- all customers have included their government ID in the registration process\n",
    "- price of subscription to halfway there is the average price of half-bottle of wine in the menu\n",
    "- bottle is delivered on Wednesday regardless if the customer has ordered a meal that day or not\n",
    "- upon registration, each customer gave a phone number (required for registration) and optionally gave an email address\n",
    "- each customer was able to set their preferred contact method (optional). If not set, customers will be contacted via SMS if their phone number was mobile, and via a direct sales call if their phone number was a landline.\n",
    "\n",
    "Data Quality issues:\n",
    "- cancellations before noon and after noon: dictionary says after noon is after 3pm. Are there 3 hours of data for cancellations missing or is the description in dictionary wrong?\n",
    "\n",
    "***\n",
    "<strong> Outline: </strong>\n",
    "1. Part 1: Exploratory Data Analysis\n",
    "2. Part 2: Build a machine learning model to predict cross-sell success \n",
    "3. Part 3: Evaluating Model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# importing libraries\n",
    "import pandas            as pd                       # data science essentials\n",
    "import numpy             as np\n",
    "import matplotlib.pyplot as plt                      # data visualization\n",
    "import seaborn           as sns                      # enhanced data viz\n",
    "import statsmodels.formula.api as smf                # statsmodel regressions\n",
    "from sklearn.model_selection import train_test_split # train-test split\n",
    "from sklearn.linear_model import LogisticRegression  # logistic regression\n",
    "from sklearn.metrics import confusion_matrix         # confusion matrix\n",
    "from sklearn.metrics import roc_auc_score            # auc score\n",
    "from sklearn.neighbors import KNeighborsClassifier   # KNN for classification\n",
    "from sklearn.neighbors import KNeighborsRegressor    # KNN for regression\n",
    "from sklearn.preprocessing import StandardScaler     # standard scaler\n",
    "from sklearn.model_selection import GridSearchCV     # hyperparameter tuning\n",
    "from sklearn.metrics import make_scorer              # customizable scorer\n",
    "from sklearn.ensemble import RandomForestClassifier     # random forest\n",
    "from sklearn.ensemble import GradientBoostingClassifier # gbm\n",
    "\n",
    "# CART model packages\n",
    "from sklearn.tree import DecisionTreeClassifier      # classification trees\n",
    "from sklearn.tree import export_graphviz             # exports graphics\n",
    "from sklearn.externals.six import StringIO           # saves objects in memory\n",
    "from IPython.display import Image                    # displays on frontend\n",
    "import pydotplus                                     # interprets dot objects\n",
    "\n",
    "# setting pandas print options\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# specifying file name\n",
    "file = \"Apprentice_Chef_Dataset.xlsx\"\n",
    "\n",
    "# reading the file into Python\n",
    "original_df = pd.read_excel(file)\n",
    "chef_org = original_df.copy()\n",
    "\n",
    "# Reading data dictionary\n",
    "chef_description = pd.read_excel('Apprentice_Chef_Data_Dictionary.xlsx')\n",
    "#chef_description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User-defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "##########  User-Defined Functions    ###########\n",
    "#################################################\n",
    "\n",
    "##########  Defining function for distribution histograms\n",
    "def distributions(variable, data, bins = 'fd', kde = False, rug = False):\n",
    "    \"\"\"\n",
    "    This function can be used for continuous or count variables.\n",
    "\n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    variable : str, continuous or count variable\n",
    "    data     : DataFrame\n",
    "    bins     : argument for matplotlib hist(), optional. If unspecified, Freedmanâ€“Diaconis rule is used.\n",
    "    kde      : bool, optional, plot or not a kernel density estimate.  If unspecified, not calculated.\n",
    "    rug      : bool, optional, include a rug on plot or not. If unspecified, not shown.\n",
    "    \"\"\"\n",
    "    \n",
    "    sns.distplot(data[variable],  \n",
    "                    bins  = bins,\n",
    "                    kde   = False,\n",
    "                    rug   = rug)\n",
    "    \n",
    "    plt.xlabel(variable)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "##########  Defining function to flag high outliers in variables\n",
    "def outlier_flag_hi(variable, threshold, data):\n",
    "    \"\"\"\n",
    "    This function is used to flag high outliers in a dataframe the variables' \n",
    "    outliers by creating a new column that is preceded by 'out_'.\n",
    "\n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    variable  : str, continuous variable.\n",
    "    threshold : float, value that will identify where outliers would be.\n",
    "    data      : dataframe, where the variables are located.\n",
    "    \n",
    "    \"\"\"\n",
    "    # creating a new column\n",
    "    data['out_' + variable + '_hi'] = 0\n",
    "        \n",
    "    # defining outlier condition\n",
    "    high = data.loc[0:,'out_' + variable + '_hi'][data[variable] > threshold]\n",
    "        \n",
    "    # imputing 1 inside flag column\n",
    "    data['out_' + variable + '_hi'].replace(to_replace = high,\n",
    "                                    value   = 1,\n",
    "                                    inplace = True)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "##########  Defining function to flag high outliers in variables\n",
    "def outlier_flag_lo(variable, threshold, data):\n",
    "    \"\"\"\n",
    "    This function is used to flag low outliers in a dataframe the variables' \n",
    "    outliers by creating a new column that is preceded by 'out_'.\n",
    "\n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    variable  : str, continuous variable.\n",
    "    threshold : float, value that will identify where outliers would be.\n",
    "    data      : dataframe, where the variables are located.\n",
    "    \n",
    "    \"\"\"\n",
    "    # creating a new column\n",
    "    data['out_' + variable + '_lo'] = 0\n",
    "        \n",
    "    # defining outlier condition\n",
    "    low = data.loc[0:,'out_' + variable + '_lo'][data[variable] < threshold]\n",
    "        \n",
    "    # imputing 1 inside flag column\n",
    "    data['out_' + variable + '_lo'].replace(to_replace = low,\n",
    "                                    value   = 1,\n",
    "                                    inplace = True)\n",
    "    \n",
    "\n",
    "##########  Defining function to plot relationships with categorical response variable\n",
    "def trend_boxplots(cont_var, response, data):\n",
    "    \"\"\"\n",
    "    This function can be used for categorical variables as target and \n",
    "    continuous variables as explanatory.\n",
    "\n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    cont_var : str, explanatory variable\n",
    "    response : str, response categorical variable\n",
    "    data     : DataFrame of the response and explanatory variables\n",
    "    \"\"\"\n",
    "\n",
    "    data.boxplot(column       = cont_var,\n",
    "                 by           = response,\n",
    "                 vert         = True,\n",
    "                 patch_artist = False,\n",
    "                 meanline     = True,\n",
    "                 showmeans    = True)\n",
    "    \n",
    "    plt.grid(b=True, which='both', linestyle='-')\n",
    "    plt.suptitle(\"\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "# Defining function to flag higher variables\n",
    "def success_flag(variable, threshold, data):\n",
    "    \"\"\"\n",
    "    This function is used to flag in a dataframe the variables' trend changes \n",
    "    above a threshold by creating a new column that is preceded by 'success_'.\n",
    "\n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    variable  : str, continuous variable.\n",
    "    threshold : float, value that will identify after which the trend on variable y changes\n",
    "    data      : dataframe, where the variables are located.\n",
    "    \n",
    "    \"\"\"\n",
    "    new_column = 'success_' + variable\n",
    "    \n",
    "    # creating a new column\n",
    "    data[new_column] = 0\n",
    "        \n",
    "    # defining outlier condition\n",
    "    high = data.loc[0:,new_column][data[variable] > threshold]\n",
    "        \n",
    "    # imputing 1 inside flag column\n",
    "    data[new_column].replace(to_replace = high,\n",
    "                             value   = 1,\n",
    "                             inplace = True)\n",
    "    \n",
    "#Defining a function to standardize numerical variables in the dataset:\n",
    "def standard(num_df):\n",
    "    \"\"\"\n",
    "    This function standardizes a dataframe that contains variables which are either\n",
    "    integers or floats.\n",
    "    \n",
    "    ------\n",
    "    num_df : DataFrame, must contain only numerical variables\n",
    "    \n",
    "    \"\"\"\n",
    "    # INSTANTIATING a StandardScaler() object\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # FITTING the scaler with housing_data\n",
    "    scaler.fit(num_df)\n",
    "\n",
    "    # TRANSFORMING our data after fit\n",
    "    X_scaled = scaler.transform(num_df)\n",
    "\n",
    "    # converting scaled data into a DataFrame\n",
    "    X_scaled_df = pd.DataFrame(X_scaled)\n",
    "    \n",
    "    # adding labels to the scaled DataFrame\n",
    "    X_scaled_df.columns = num_df.columns\n",
    "    \n",
    "    # Re-attaching target variable to DataFrame\n",
    "    #X_scaled_df = X_scaled_df.join(target_variable)\n",
    "    \n",
    "    # returning the standardized data frame into the global environment\n",
    "    return X_scaled_df\n",
    "\n",
    "# defining a function to find optimal number of neighbors in KNN\n",
    "def optimal_neighbors(criteria, X_train, y_train, X_test, y_test, max_neighbors):\n",
    "    \"\"\"\n",
    "    This function calculates and returns the optimal number of neighbors for a \n",
    "    KNN classifier the optimal number of neighbors given a training, testing, \n",
    "    a random seed and max. Should only be used with standardized data.\n",
    "    \n",
    "    ----\n",
    "    criteria      : str, score to which return the optimal number of neighbors.\n",
    "                        'accuracy', 'auc'\n",
    "    X_train       : set of explanatory training data\n",
    "    y_train       : set of target training data\n",
    "    X_test.       : set of explanatory testing data\n",
    "    y_test        : set of target testing data\n",
    "    max_neighbors : maximum number of neighbors to be tested\n",
    "    \"\"\"\n",
    "    # creating empty lists for training set accuracy, test set accuracy and AUC\n",
    "    training_accuracy = []\n",
    "    test_accuracy     = []\n",
    "    auc_score         = []\n",
    "    \n",
    "    neighbors_settings = range(1, max_neighbors)\n",
    "    \n",
    "    for n_neighbors in neighbors_settings:\n",
    "        #Building Model\n",
    "        clf = KNeighborsClassifier(n_neighbors = n_neighbors)\n",
    "        clf.fit(X_train, y_train.values.reshape(-1,))\n",
    "        clf_pred = clf.predict(X_test)\n",
    "        \n",
    "        # Recording scores\n",
    "        training_accuracy.append(clf.score(X_train,y_train))\n",
    "        test_accuracy.append(clf.score(X_test,y_test))\n",
    "        auc_score.append(roc_auc_score(y_true = y_test,\n",
    "                                       y_score = clf_pred))\n",
    "    \n",
    "    opt_neighbors_accuracy = test_accuracy.index(max(test_accuracy)) + 1\n",
    "    opt_neighbors_auc      = auc_score.index(max(auc_score)) + 1\n",
    "    #returning the optimal number of neighbors\n",
    "    if criteria == 'accuracy':\n",
    "        return opt_neighbors_accuracy\n",
    "    elif criteria == 'auc':\n",
    "        return opt_neighbors_auc\n",
    "    else:\n",
    "        print(\"\"\"Error: criteria specified not available. Argument can only take 'accuracy' or 'auc' \"\"\")\n",
    "\n",
    "# classification confusion matrix\n",
    "def visual_cm(true_y, pred_y, labels = None, title = 'Confusion Matrix of the Classifier'):\n",
    "    \"\"\"\n",
    "Creates a visualization of a confusion matrix.\n",
    "\n",
    "PARAMETERS\n",
    "----------\n",
    "true_y : true values for the response variable\n",
    "pred_y : predicted values for the response variable\n",
    "labels : , default None\n",
    "titel  : str, title of confusion matrix, default: 'Confusion Matrix of the Classifier'\n",
    "    \"\"\"\n",
    "    # visualizing the confusion matrix\n",
    "\n",
    "    # setting labels\n",
    "    lbls = labels\n",
    "    \n",
    "\n",
    "    # declaring a confusion matrix object\n",
    "    cm = confusion_matrix(y_true = true_y,\n",
    "                          y_pred = pred_y)\n",
    "\n",
    "\n",
    "    # heatmap\n",
    "    sns.heatmap(cm,\n",
    "                annot       = True,\n",
    "                xticklabels = lbls,\n",
    "                yticklabels = lbls,\n",
    "                cmap        = 'Blues',\n",
    "                fmt         = 'g')\n",
    "\n",
    "\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<strong> Observations: </strong>\n",
    "- Data types are coherent with each variable description\n",
    "- 47 missing values in Family Name\n",
    "- Number of observations: 1946\n",
    "- Total of 28 variables (including target variable) where:\n",
    "    - 3 are floats\n",
    "    - 22 are integers\n",
    "    - 4 are objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## Part 1: Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, our objective is too understand the data and identify possible insights that will be useful for business application. We'll also go through feature engineering (creating new variables) were we deem appropriate.\n",
    "<br> <br>\n",
    "For this purpose, it is important to identify the different variable types in our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining lists for each type of variable:\n",
    "categorical = ['CROSS_SELL_SUCCESS',     # (target variable - binary)\n",
    "               'MOBILE_NUMBER',          # (also binary)\n",
    "               'TASTES_AND_PREFERENCES', # (also binary)\n",
    "               'PACKAGE_LOCKER',         # (also binary)\n",
    "               'REFRIGERATED_LOCKER']    # (also binary)\n",
    "\n",
    "continuous = ['REVENUE',\n",
    "              'AVG_TIME_PER_SITE_VISIT',\n",
    "              'FOLLOWED_RECOMMENDATIONS_PCT',\n",
    "              'AVG_PREP_VID_TIME',\n",
    "              'MEDIAN_MEAL_RATING',]     # (interval)\n",
    "\n",
    "counts = ['TOTAL_MEALS_ORDERED',\n",
    "          'UNIQUE_MEALS_PURCH',\n",
    "          'CONTACTS_W_CUSTOMER_SERVICE',\n",
    "          'PRODUCT_CATEGORIES_VIEWED',\n",
    "          'CANCELLATIONS_BEFORE_NOON',\n",
    "          'CANCELLATIONS_AFTER_NOON',\n",
    "          'MOBILE_LOGINS',\n",
    "          'PC_LOGINS',\n",
    "          'WEEKLY_PLAN',\n",
    "          'EARLY_DELIVERIES',\n",
    "          'LATE_DELIVERIES',\n",
    "          'MASTER_CLASSES_ATTENDED',\n",
    "          'TOTAL_PHOTOS_VIEWED',\n",
    "          'LARGEST_ORDER_SIZE',\n",
    "          'AVG_CLICKS_PER_VISIT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Anomaly Detection: Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Purpose: </strong> Identify and create a strategy for missing values.\n",
    "***\n",
    "Missing values can affect our model and our ability to create plots to identify other features in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inspecting missing values\n",
    "#chef_org.isnull().sum()\n",
    "#chef_org.loc[:,:][chef_org['FAMILY_NAME'].isna()]\n",
    "\n",
    "# Flagging missing variables for FAMILY_NAME\n",
    "# creating a copy of dataframe for safety measures\n",
    "chef_m = chef_org.copy()\n",
    "\n",
    "# creating a new column where 1 indicates that observation has a missing family name\n",
    "chef_m['m_FAMILY_NAME'] = chef_m['FAMILY_NAME'].isnull().astype(int)\n",
    "\n",
    "# imputing missing values\n",
    "chef_m['FAMILY_NAME'] = chef_m['FAMILY_NAME'].fillna('Unknown')\n",
    "\n",
    "# checking to see if missing values were imputed\n",
    "#chef_m.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Anomaly Detection: Sample Size Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Purpose: </strong> Identify size of each category in categorical variables.\n",
    "\n",
    "***\n",
    "We need to see if the size of each of the categories is large enough to infer statistical significance or insignificance. If not, the variable could be insignificant to predict cross sell success when in reality its sample size is too small. <br> <br>\n",
    "Additionally, since our target variable is binary, we want to ensure we have the same number of success and failure values in our sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for variable in chef_org:\n",
    "#    if variable in categorical:\n",
    "#        print(f\"\"\"{variable}\n",
    "#------\n",
    "#{chef_org[variable].value_counts()}\n",
    "#          \n",
    "#    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Observations: </strong>\n",
    "- Sample size for each option in all categorical variables are large enough for analysis (all contain above 200 observations)\n",
    "- Sample size for target variable is large enough. We will need to use stratification methods in our splitting of training and testing in the model to make sure we have both success and failures in both training and testing.\n",
    "<br><br> \n",
    "In this sample of customers, our <strong> success in selling Halfway There was about 88% ! </strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) Anomaly Detection: Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Purpose: </strong> Outliers affect most predictive models. It increases variance in a variable, and therefore need to be flagged for two main reasons: <br> <br>\n",
    "1) Using outlier flag variable in our model quantifies the affect of that outlier on the variable we are trying to predict (in this case, cross sell success) <br>\n",
    "2) In some cases, removing outliers can improve our predictions and increase generalization of our model <br> <br>\n",
    "\n",
    "***\n",
    "\n",
    "<nbsp> <nbsp> <nbsp> <nbsp>  In the following code, we visualize each variable's distribution with an user-defined function and we look at the quartile ranges using descriptive statistics. We then set thresholds which will determine which observations are going to be considered as outliers in this analysis. Finally, we create a new column for each of the variables that contain outliers, where a 1 will be imputed for outlier observations. <br><br>\n",
    "<i> Note: no outliers are removed in the part of the analysis <i> <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualizing variable distributions\n",
    "#for variable in continuous + counts:\n",
    "#    distributions(variable, chef_m, bins = 'fd', kde = True, rug = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Observations:</strong>\n",
    "- Revenue: big dip in clients with revenue at approx 2,000 \n",
    "- Avg Time per Site Visit (in seconds): almost a normal distribution, outliers after 200 (3.3 min)    \n",
    "- Followed Recommendations Percentage: outliers after 80%  and before 10%   \n",
    "- Average Preparation Video Time (in seconds): almost a normal distribution, outliers after 250 (approx 4 min)    \n",
    "- Largest Order Size: almost normal distribution, after 5: a family is usually 4 - 5 people, more than that it could be that these customers are throwing dinner parties or keeping the meals for the next day\n",
    "- Median Meal Rating: peak on 3, no obvious outliers\n",
    "- Average Clicks per visit: outliers before 10\n",
    "- Total Meals Ordered: strong dip in around 25 - investigate, outliers after 320    \n",
    "- Unique Meals Purchased: outliers after 10    \n",
    "- Contacts with customer service: outliers after 13    \n",
    "- Product Categories Viewed: after 9 and before 2\n",
    "- Cancellations Before Noon: approx exponential distribution, outliers after 8   \n",
    "- Cancellations After Noon: no obvious outliers   \n",
    "- Mobile Log-ins: no obvious outliers  \n",
    "- PC Log-ins: no obvious outliers\n",
    "- Weekly Plan: no obvious outliers\n",
    "- Early Deliveries: peak on 0, no obvious outliers\n",
    "- Late Deliveries: outliers after 17   \n",
    "- Master Class Attended: no obvious outliers  \n",
    "- Total Photos Viewed: peak on 0, outliers after 800   \n",
    "- Revenue per meal: Outliers after 80 dollars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establishing outliers thresholds for analysis\n",
    "# Continous\n",
    "avg_time_per_site_visit_hi = 200\n",
    "avg_prep_vid_time_hi       = 250\n",
    "followed_rec_hi            = 75\n",
    "followed_rec_lo            = 10 \n",
    "largest_order_size_hi      = 5\n",
    "avg_clicks_per_visit_hi    = 17\n",
    "avg_clicks_per_visit_lo    = 11\n",
    "median_meal_hi             = 3\n",
    "\n",
    "# Counts:\n",
    "total_meals_ordered_hi            = 320\n",
    "unique_meals_purchased_hi         = 8\n",
    "unique_meals_purchased_lo         = 2\n",
    "contacts_with_customer_service_hi = 13\n",
    "cancellations_before_noon_hi      = 8\n",
    "late_deliveries_hi                = 17\n",
    "total_photos_viewed_hi            = 800\n",
    "products_viewed_hi                = 9 \n",
    "products_viewed_lo                = 2 \n",
    "median_meal_lo                    = 2\n",
    "\n",
    "# Target Variable\n",
    "revenue_hi  =  5500\n",
    "\n",
    "\n",
    "# Creating Dictionary to link variables with outlier thresholds\n",
    "lst_thresholds_hi = {\n",
    "    'AVG_TIME_PER_SITE_VISIT'      : avg_time_per_site_visit_hi,\n",
    "    'AVG_PREP_VID_TIME'            : avg_prep_vid_time_hi,\n",
    "    'TOTAL_MEALS_ORDERED'          : total_meals_ordered_hi,\n",
    "    'UNIQUE_MEALS_PURCH'           : unique_meals_purchased_hi,\n",
    "    'CONTACTS_W_CUSTOMER_SERVICE'  : contacts_with_customer_service_hi,\n",
    "    'CANCELLATIONS_BEFORE_NOON'    : cancellations_before_noon_hi,\n",
    "    'LATE_DELIVERIES'              : late_deliveries_hi,\n",
    "    'TOTAL_PHOTOS_VIEWED'          : total_photos_viewed_hi,\n",
    "    'REVENUE'                      : revenue_hi,\n",
    "    'FOLLOWED_RECOMMENDATIONS_PCT' : followed_rec_hi,\n",
    "    'LARGEST_ORDER_SIZE'           : largest_order_size_hi,\n",
    "    'PRODUCT_CATEGORIES_VIEWED'    : products_viewed_hi,\n",
    "    'AVG_CLICKS_PER_VISIT'         : avg_clicks_per_visit_hi,\n",
    "    'PRODUCT_CATEGORIES_VIEWED'    : products_viewed_hi,\n",
    "    'MEDIAN_MEAL_RATING'           : median_meal_hi\n",
    "    }\n",
    "\n",
    "lst_thresholds_lo = {\n",
    "    'AVG_CLICKS_PER_VISIT'          : avg_clicks_per_visit_lo,\n",
    "    'PRODUCT_CATEGORIES_VIEWED'     : products_viewed_lo,\n",
    "    'FOLLOWED_RECOMMENDATIONS_PCT'  : followed_rec_lo,\n",
    "    'UNIQUE_MEALS_PURCH'            : unique_meals_purchased_lo,\n",
    "    'MEDIAN_MEAL_RATING'            : median_meal_lo\n",
    "     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a copy of dataframe for safety measures\n",
    "chef_o = chef_m.copy()\n",
    "\n",
    "# Looping over variables to create outlier flags:\n",
    "for key in lst_thresholds_hi.keys():\n",
    "    outlier_flag_hi(key,lst_thresholds_hi[key],chef_o)\n",
    "    \n",
    "for key in lst_thresholds_lo.keys():\n",
    "    outlier_flag_lo(key,lst_thresholds_lo[key],chef_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging avg clicks per visit hi and lo\n",
    "chef_o['out_AVG_CLICKS_PER_VISIT'] = chef_o['out_AVG_CLICKS_PER_VISIT_hi'] + chef_o['out_AVG_CLICKS_PER_VISIT_lo'] \n",
    "\n",
    "#chef_o.loc[:, ['out_AVG_CLICKS_PER_VISIT','out_AVG_CLICKS_PER_VISIT_hi','out_AVG_CLICKS_PER_VISIT_lo']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D) Feature Engineering: Email Domains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Purpose </strong>: \n",
    "<br>\n",
    "When we promote Halfway There to a wider customer base, we could choose from several promotion methods (ex: sales call, flyers, email, SMS...). With our customers email domains, we can identify if the email provided in the application process is a professional or personal email, or if they have provided a 'junk' email (an inbox they never open but use to avoid spam). \n",
    "<br>\n",
    "***\n",
    "\n",
    "By adding these features in our analysis, we are able to identify if customers that use their personal or professional emails are more likely to buy the subscription. If so, it would be a good idea to implement an email marketing campaign to these customers. It would also confirm the need to run a campaign in another platform if we see the potential in customers with 'junk' emails.\n",
    "<br> <br>\n",
    "In the next steps, we will first select the email domain for each customer, then create a new categorical variable where each domain is classified as \"personal\", \"professional\" or \"junk\". Finally, we will be one-hot encoding this new variable which will create three new columns for each email category. In these new columns, if an email corresponds to the column, that observation will take on the value 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: splitting emails\n",
    "# placeholder list\n",
    "placeholder_lst = []\n",
    "\n",
    "# looping over each email address\n",
    "for index, col in chef_o.iterrows():\n",
    "    \n",
    "    # splitting email domain at '@'\n",
    "    split_email = chef_o.loc[index, 'EMAIL'].split(sep = '@')\n",
    "\n",
    "    # appending placeholder_lst with the results\n",
    "    placeholder_lst.append(split_email)\n",
    "    \n",
    "# converting placeholder_lst into a DataFrame\n",
    "email_df = pd.DataFrame(placeholder_lst)\n",
    "\n",
    "# STEP 2: concatenating with original DataFrame\n",
    "# Creating a copy of chef for features and safety measure\n",
    "chef_v = chef_o.copy()\n",
    "\n",
    "# renaming column to concatenate\n",
    "email_df.columns = ['name' , 'EMAIL_DOMAIN'] \n",
    "\n",
    "# concatenating personal_email_domain with chef DataFrame\n",
    "chef_v = pd.concat([chef_v, email_df.loc[:, 'EMAIL_DOMAIN']], \n",
    "                   axis = 1)\n",
    "\n",
    "# printing value counts of personal_email_domain\n",
    "chef_v.loc[: ,'EMAIL_DOMAIN'].value_counts()\n",
    "\n",
    "# email domain types\n",
    "professional_email_domains = ['@mmm.com',         '@amex.com',\n",
    "                              '@apple.com',       '@boeing.com',\n",
    "                              '@caterpillar.com', '@chevron.com',\n",
    "                              '@cisco.com',       '@cocacola.com',\n",
    "                              '@disney.com',      '@dupont.com',\n",
    "                              '@exxon.com',       '@ge.org',\n",
    "                              '@goldmansacs.com', '@homedepot.com',\n",
    "                              '@ibm.com',         '@intel.com',\n",
    "                              '@jnj.com',         '@jpmorgan.com',\n",
    "                              '@mcdonalds.com',   '@merck.com',\n",
    "                              '@microsoft.com',   '@nike.com',\n",
    "                              '@pfizer.com',      '@pg.com',\n",
    "                              '@travelers.com',   '@unitedtech.com',\n",
    "                              '@unitedhealth.com','@verizon.com',\n",
    "                              '@visa.com',        '@walmart.com']\n",
    "personal_email_domains     = ['@gmail.com',       '@yahoo.com',    \n",
    "                              '@protonmail.com']\n",
    "junk_email_domains         = ['@me.com',          '@aol.com',\n",
    "                              '@hotmail.com',     '@live.com', \n",
    "                              '@msn.com',         '@passport.com']\n",
    "\n",
    "# placeholder list\n",
    "placeholder_lst = []  \n",
    "\n",
    "\n",
    "# looping to group observations by domain type\n",
    "for domain in chef_v['EMAIL_DOMAIN']:\n",
    "        if \"@\" + domain in professional_email_domains:\n",
    "            placeholder_lst.append('professional')\n",
    "            \n",
    "        elif \"@\" + domain in personal_email_domains:\n",
    "            placeholder_lst.append('personal')\n",
    "            \n",
    "        elif \"@\" + domain in junk_email_domains:\n",
    "            placeholder_lst.append('junk')\n",
    "            \n",
    "        else:\n",
    "            print('Unknown')\n",
    "\n",
    "\n",
    "# concatenating with original DataFrame\n",
    "chef_v['email_domain_group'] = pd.Series(placeholder_lst)\n",
    "\n",
    "# checking results and sample size\n",
    "#print(chef['email_domain_group'].value_counts())\n",
    "\n",
    "# Step 3: One-Hot encoding\n",
    "one_hot_email_domain = pd.get_dummies(chef_v['email_domain_group'])\n",
    "\n",
    "# dropping orginal columns to keep only encoded ones\n",
    "chef_e               = chef_v.drop(['email_domain_group','EMAIL','EMAIL_DOMAIN'], axis = 1)\n",
    "\n",
    "# joining encoded columns to dataset\n",
    "chef_e               = chef_e.join(one_hot_email_domain)\n",
    "\n",
    "# including new categorical variables to list\n",
    "domains              = ['professional','personal','junk']\n",
    "\n",
    "### Only run once!\n",
    "#categorical          = categorical + domains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D) Feature Engineering: Computing New Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Purpose: </strong> Our analysis can benefit from the creation of new features based on the data we already have. <br>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Feature: </strong> Revenue per meal <br> <br>\n",
    "Our goal in creating Halfway There is to diversify revenue. This feature will hopefully shed a light on how much a customer spends on a meal. Hopefully, we can identify certain customer segments according to this new variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a copy of dataframe for safety measures\n",
    "chef_n = chef_e.copy()\n",
    "\n",
    "# placeholder for 'rev_per_meal' feature\n",
    "chef_n['rev_per_meal'] = 0\n",
    "\n",
    "# replacing values based on calculation\n",
    "for index, col in chef_n.iterrows():\n",
    "    revenue      = chef_n.loc[index, 'REVENUE']\n",
    "    total_orders = chef_n.loc[index, 'TOTAL_MEALS_ORDERED']\n",
    "    chef_n.loc[index, 'rev_per_meal'] = (revenue / total_orders).round(2)\n",
    "        \n",
    "# checking results\n",
    "#chef_n.loc[0:10,['rev_per_meal', 'REVENUE', 'TOTAL_MEALS_ORDERED']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating our variables list after new features (only run once)\n",
    "#continuous.append('rev_per_meal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining Outliers in new variable\n",
    "#distributions('rev_per_meal', chef_n)\n",
    "\n",
    "# Establishing Outlier Flags\n",
    "rev_per_meal_hi = 70\n",
    "rev_per_meal_lo = 15\n",
    "outlier_flag_hi('rev_per_meal', rev_per_meal_hi, chef_n)\n",
    "outlier_flag_lo('rev_per_meal', rev_per_meal_lo, chef_n)\n",
    "\n",
    "#chef_n.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Feature: </strong> Revenue per logins <br> <br>\n",
    "When launching a new product, an important question is what channels to market the product in. Therefore, computing the revenue per logins for mobile and pc logins could help us understand user's buying behavior and better tailor our marketing strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a copy of dataframe for safety measures\n",
    "chef_n = chef_n.copy()\n",
    "\n",
    "# new column for 'rev_per_login' feature\n",
    "chef_n['rev_per_pclogin']     = 0\n",
    "chef_n['rev_per_mobilelogin'] = 0\n",
    "\n",
    "# replacing values based on calculation\n",
    "for index, col in chef_n.iterrows():\n",
    "    revenue       = chef_n.loc[index, 'REVENUE']\n",
    "    PC_LOGINS     = chef_n.loc[index, 'PC_LOGINS']\n",
    "    if PC_LOGINS   == 0:\n",
    "        chef_n.loc[index, 'rev_per_pclogin'] = 0\n",
    "    elif PC_LOGINS >= 0:\n",
    "        chef_n.loc[index, 'rev_per_pclogin'] = (revenue / PC_LOGINS).round(2)\n",
    "    else:\n",
    "        print('Something went wrong.')\n",
    "\n",
    "for index, col in chef_n.iterrows():\n",
    "    revenue       = chef_n.loc[index, 'REVENUE']\n",
    "    MOBILE_LOGINS = chef_n.loc[index, 'MOBILE_LOGINS']    \n",
    "    if MOBILE_LOGINS   == 0:\n",
    "        chef_n.loc[index, 'rev_per_mobilelogin'] = 0\n",
    "    elif MOBILE_LOGINS >= 0:\n",
    "        chef_n.loc[index, 'rev_per_mobilelogin'] = (revenue / MOBILE_LOGINS).round(2)\n",
    "    else:\n",
    "        print('Something went wrong.')\n",
    "\n",
    "# checking results\n",
    "#chef_n.loc[0:10,['rev_per_mobilelogin', 'REVENUE', 'rev_per_pclogin']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Determining Outliers in new variable\n",
    "#distributions('rev_per_pclogin', chef_n)\n",
    "\n",
    "# flagging outliers\n",
    "rev_per_pclogin_hi = 800\n",
    "rev_per_pclogin_lo = 150\n",
    "outlier_flag_hi('rev_per_pclogin', rev_per_pclogin_hi, chef_n)\n",
    "outlier_flag_lo('rev_per_pclogin', rev_per_pclogin_lo, chef_n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining Outliers in new variable\n",
    "#distributions('rev_per_mobilelogin', chef_n)\n",
    "\n",
    "# flagging outliers\n",
    "rev_per_mobilelogin_hi = 2500\n",
    "rev_per_mobilelogin_lo = 200\n",
    "outlier_flag_hi('rev_per_mobilelogin', rev_per_mobilelogin_hi, chef_n)\n",
    "outlier_flag_lo('rev_per_mobilelogin', rev_per_mobilelogin_lo, chef_n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E) Feature Engineering: Trend Based Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Purpose: </strong> Identify points in the relationships between explanatory variables and response variable where there is a clear separation between a success case and failure case.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changes in variables behaviors in relation to our target variable can provide powerful explanations in our model as to how our success in selling Halfway There might be affected.\n",
    "<br> <br>\n",
    "In the following code, we visualize each variable's relationship with cross sell success with an user-defined function. We then set thresholds which will determine which observations are going to be considered having a differing effect on our success in selling Halfway There. Finally, we create a new feature for each of the variables that contain this difference, where a 1 will be imputed for these observations. <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## calling the function for each categorical variable\n",
    "#for var in continuous + counts:\n",
    "#    trend_boxplots(cont_var = var,\n",
    "#                   response = 'CROSS_SELL_SUCCESS',\n",
    "#                   data     = chef_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Observations </strong>\n",
    "(clear separations between success and failure):\n",
    "- Avg time per site visit: huge outlier. After removing it still no clear separation between success and failure.\n",
    "- Followed Recommendation Pct: high separation - low quartile of 1 is higher quartile of 0 \n",
    "- Cancellations before noon: high separation - median for 1 is high quartile for 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Further investigation: </strong>\n",
    "- cancellations_before_noon\n",
    "- followed recommendations percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "928\n",
      "351\n"
     ]
    }
   ],
   "source": [
    "# Cancellations Before Noon Analysis\n",
    "# count of cancellations before noon that subscribe to new service\n",
    "did = chef_n.loc[:,'CANCELLATIONS_BEFORE_NOON'][chef_n.loc[:,'CROSS_SELL_SUCCESS'] == 1]\\\n",
    "                                                     [chef_n.loc[:,'CANCELLATIONS_BEFORE_NOON'] > 0]\n",
    "\n",
    "# count of cancellations before noon that did not subscribe to new service\n",
    "did_not = chef_n.loc[:,'CANCELLATIONS_BEFORE_NOON'][chef_n.loc[:,'CROSS_SELL_SUCCESS'] == 0]\\\n",
    "                                                   [chef_n.loc[:,'CANCELLATIONS_BEFORE_NOON'] > 0]\n",
    "print(len(did))\n",
    "print(len(did_not))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73.01337529504328"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((928/(920+351))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> INSIGHT: </strong> Of those that have cancelled at least once before noon, 73% have subscribed to Halfway There.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "877\n",
      "149\n"
     ]
    }
   ],
   "source": [
    "# Followed Recommendation Percentage Analysis\n",
    "# count of Followed Recommendation Percentage that subscribe to new service\n",
    "did = chef_n.loc[:,'FOLLOWED_RECOMMENDATIONS_PCT'][chef_n.loc[:,'CROSS_SELL_SUCCESS'] == 1]\\\n",
    "                                                     [chef_n.loc[:,'FOLLOWED_RECOMMENDATIONS_PCT'] > 20]\n",
    "\n",
    "# count of cancellations before noon that did not subscribe to new service\n",
    "did_not = chef_n.loc[:,'FOLLOWED_RECOMMENDATIONS_PCT'][chef_n.loc[:,'CROSS_SELL_SUCCESS'] == 0]\\\n",
    "                                                   [chef_n.loc[:,'FOLLOWED_RECOMMENDATIONS_PCT'] > 20]\n",
    "print(len(did))\n",
    "print(len(did_not))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.854775828460039"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "877/(877+149)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> INSIGHT: </strong> Of those that have followed a recommendation more than 20%, 85% have subscribed to Halfway There.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<strong> Feature engineering: </strong> creating flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establishing trend thresholds for analysis\n",
    "# above this threshold its a succes\n",
    "followed_recommendations_pct_1 = 20 #(or 30 for certainty)\n",
    "cancellations_before_noon_1    = 2 #(or 1 for mean)\n",
    "median_ratings_1               = 3\n",
    "median_ratings_2               = 2\n",
    "\n",
    "# Creating Dictionary to link variables with outlier thresholds\n",
    "success_trend = {\n",
    "    'FOLLOWED_RECOMMENDATIONS_PCT' : followed_recommendations_pct_1,\n",
    "    'CANCELLATIONS_BEFORE_NOON'    : cancellations_before_noon_1,\n",
    "    'MEDIAN_MEAL_RATING'           : median_ratings_1,\n",
    "    'MEDIAN_MEAL_RATING'           : median_ratings_2\n",
    "     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a copy of dataframe for safety measures\n",
    "chef_t = chef_n.copy()\n",
    "\n",
    "# Looping over variables to create trend flags:\n",
    "for key in success_trend.keys():\n",
    "    success_flag(key,success_trend[key],chef_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F) Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_corr = chef_t.corr().round(2)\n",
    "#df_corr['CROSS_SELL_SUCCESS'].sort_values(ascending = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Observations: </strong>\n",
    "- Junk email is negatively correlated to cross sell success <strong>(-0.28)</strong> -> if communications of the promotion were by email, then this makes sense, since junk emails are people that have other emails as to not receive promotions. \n",
    "- Professional <strong>(+0.19)</strong> -> people with these emails are more likely to subscribe, if communications about the subscription are through email, these are the ones that are going to open and click on it\n",
    "<br><br>\n",
    "--> because email communication is optional (i.e. only happens if the customer set their preferred method as email and not mobile which is default), then this correlation does not provide significant insight. Customers may have used a junk email because they prefer to receive sms and are as responsive to that as they would be on a professional email. More information is needed for a complete analysis.\n",
    "<br> <br>\n",
    "- Followed recommendations percentage highly correlated to success <strong>(+ 0.46) </strong> -> people that follow a lot of recommendations on the meal suggestions will most likely subscribe to the wine service\n",
    "- Cancellations before noon <strong> (+0.16) </strong> -> not a high correlation but interesting. Hypothesis: \n",
    "    - Customers that cancel before noon might have a busier than usual schedule so committing to a meal delivery is harder than committing to a just half a bottle of wine (does not need to be drank with the meal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen previously, in classification models we need to ensure our target variable is balanced in terms of success and failure cases. If not, it could affect our model predictions. <br>\n",
    "\n",
    "When splitting the data between training and testing sets, we need to ensure both cases are represented. We will do so through stratification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation: Data Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a copy for safety measures\n",
    "chef = chef_t.copy()\n",
    "\n",
    "# dropping discrete variables (only run once!)\n",
    "chef = chef.drop(['NAME', 'FIRST_NAME', 'FAMILY_NAME'], axis = 1)\n",
    "\n",
    "# checking the results\n",
    "#chef.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "code_folding": [
     5,
     15,
     76,
     95,
     111,
     124,
     138
    ]
   },
   "outputs": [],
   "source": [
    "# Defining a dictionary with explanatory variables names \n",
    "variables_dict = {\n",
    "    \"target\"     : [    # target variable\n",
    "        'CROSS_SELL_SUCCESS'\n",
    "    ],\n",
    "    \"Base\"       :  [   # dataset without feature engineering\n",
    "        'REVENUE', 'TOTAL_MEALS_ORDERED', 'UNIQUE_MEALS_PURCH',\n",
    "        'CONTACTS_W_CUSTOMER_SERVICE',    'PRODUCT_CATEGORIES_VIEWED', \n",
    "        'AVG_TIME_PER_SITE_VISIT', 'MOBILE_NUMBER', 'CANCELLATIONS_BEFORE_NOON', \n",
    "        'CANCELLATIONS_AFTER_NOON', 'TASTES_AND_PREFERENCES', 'PC_LOGINS',\n",
    "        'MOBILE_LOGINS', 'WEEKLY_PLAN', 'EARLY_DELIVERIES', 'LATE_DELIVERIES', \n",
    "        'PACKAGE_LOCKER', 'REFRIGERATED_LOCKER', 'FOLLOWED_RECOMMENDATIONS_PCT',\n",
    "        'AVG_PREP_VID_TIME', 'LARGEST_ORDER_SIZE', 'MASTER_CLASSES_ATTENDED', \n",
    "        'MEDIAN_MEAL_RATING', 'AVG_CLICKS_PER_VISIT', 'TOTAL_PHOTOS_VIEWED'\n",
    "    ],\n",
    "    \"Full Model\"  :  [\n",
    "        'REVENUE',\n",
    "        'TOTAL_MEALS_ORDERED',\n",
    "        'UNIQUE_MEALS_PURCH',\n",
    "        'CONTACTS_W_CUSTOMER_SERVICE',\n",
    "        'PRODUCT_CATEGORIES_VIEWED',\n",
    "        'AVG_TIME_PER_SITE_VISIT',\n",
    "        'MOBILE_NUMBER',\n",
    "        'CANCELLATIONS_BEFORE_NOON',\n",
    "        'CANCELLATIONS_AFTER_NOON',\n",
    "        'TASTES_AND_PREFERENCES',\n",
    "        'PC_LOGINS',\n",
    "        'MOBILE_LOGINS',\n",
    "        'WEEKLY_PLAN',\n",
    "        'EARLY_DELIVERIES',\n",
    "        'LATE_DELIVERIES',\n",
    "        'PACKAGE_LOCKER',\n",
    "        'REFRIGERATED_LOCKER',\n",
    "        'FOLLOWED_RECOMMENDATIONS_PCT',\n",
    "        'AVG_PREP_VID_TIME',\n",
    "        'LARGEST_ORDER_SIZE',\n",
    "        'MASTER_CLASSES_ATTENDED',\n",
    "        'MEDIAN_MEAL_RATING',\n",
    "        'AVG_CLICKS_PER_VISIT',\n",
    "        'TOTAL_PHOTOS_VIEWED',\n",
    "        'm_FAMILY_NAME',\n",
    "        'out_AVG_TIME_PER_SITE_VISIT_hi',\n",
    "        'out_AVG_PREP_VID_TIME_hi',\n",
    "        'out_TOTAL_MEALS_ORDERED_hi',\n",
    "        'out_UNIQUE_MEALS_PURCH_hi',\n",
    "        'out_CONTACTS_W_CUSTOMER_SERVICE_hi',\n",
    "        'out_CANCELLATIONS_BEFORE_NOON_hi',\n",
    "        'out_LATE_DELIVERIES_hi',\n",
    "        'out_TOTAL_PHOTOS_VIEWED_hi',\n",
    "        'out_REVENUE_hi',\n",
    "        'out_FOLLOWED_RECOMMENDATIONS_PCT_hi',\n",
    "        'out_LARGEST_ORDER_SIZE_hi',\n",
    "        'out_PRODUCT_CATEGORIES_VIEWED_hi',\n",
    "        'out_AVG_CLICKS_PER_VISIT_hi',\n",
    "        'out_MEDIAN_MEAL_RATING_hi',\n",
    "        'out_AVG_CLICKS_PER_VISIT_lo',\n",
    "        'out_PRODUCT_CATEGORIES_VIEWED_lo',\n",
    "        'out_FOLLOWED_RECOMMENDATIONS_PCT_lo',\n",
    "        'out_UNIQUE_MEALS_PURCH_lo',\n",
    "        'out_MEDIAN_MEAL_RATING_lo',\n",
    "        'junk',\n",
    "        'personal',\n",
    "        'professional',\n",
    "        'rev_per_meal',\n",
    "        'out_rev_per_meal_hi',\n",
    "        'out_rev_per_meal_lo',\n",
    "        'rev_per_pclogin',\n",
    "        'rev_per_mobilelogin',\n",
    "        'out_rev_per_pclogin_hi',\n",
    "        'out_rev_per_pclogin_lo',\n",
    "        'out_rev_per_mobilelogin_hi',\n",
    "        'out_rev_per_mobilelogin_lo',\n",
    "        'success_FOLLOWED_RECOMMENDATIONS_PCT',\n",
    "        'success_CANCELLATIONS_BEFORE_NOON',\n",
    "        'success_MEDIAN_MEAL_RATING'\n",
    "    ],\n",
    "    \"Important Model\"  : [   # variables from EDA\n",
    "          'EARLY_DELIVERIES',\n",
    "          'MOBILE_NUMBER','CANCELLATIONS_BEFORE_NOON',\n",
    "          'CANCELLATIONS_AFTER_NOON','TASTES_AND_PREFERENCES',\n",
    "          'REFRIGERATED_LOCKER','FOLLOWED_RECOMMENDATIONS_PCT',\n",
    "          'personal','professional','junk',\n",
    "          'out_FOLLOWED_RECOMMENDATIONS_PCT_hi',\n",
    "          'out_FOLLOWED_RECOMMENDATIONS_PCT_lo',\n",
    "          'out_PRODUCT_CATEGORIES_VIEWED_hi',\n",
    "          'out_PRODUCT_CATEGORIES_VIEWED_lo',\n",
    "          'out_MEDIAN_MEAL_RATING_hi',\n",
    "          'out_MEDIAN_MEAL_RATING_lo',\n",
    "          'rev_per_mobilelogin',\n",
    "          'out_rev_per_pclogin_hi',\n",
    "          'out_rev_per_pclogin_lo',\n",
    "          'out_rev_per_mobilelogin_hi',\n",
    "          'out_rev_per_mobilelogin_lo',\n",
    "          'success_FOLLOWED_RECOMMENDATIONS_PCT'\n",
    "    ],\n",
    "    'Full Tree Features' : [ #important features for a tree with full dataset\n",
    "        'UNIQUE_MEALS_PURCH',\n",
    "        'MOBILE_NUMBER',\n",
    "        'LARGEST_ORDER_SIZE',\n",
    "        'TOTAL_PHOTOS_VIEWED',\n",
    "        'CONTACTS_W_CUSTOMER_SERVICE',\n",
    "        'TOTAL_MEALS_ORDERED',\n",
    "        'rev_per_meal',\n",
    "        'EARLY_DELIVERIES',\n",
    "        'REVENUE',\n",
    "        'professional',\n",
    "        'CANCELLATIONS_BEFORE_NOON',\n",
    "        'rev_per_mobilelogin',\n",
    "        'junk',\n",
    "        'FOLLOWED_RECOMMENDATIONS_PCT'\n",
    "    ],\n",
    "    'Random Forest Full' : [\n",
    "        'WEEKLY_PLAN',\n",
    "        'TOTAL_MEALS_ORDERED',\n",
    "        'junk',\n",
    "        'rev_per_pclogin',\n",
    "        'REVENUE',\n",
    "        'rev_per_mobilelogin',\n",
    "        'AVG_PREP_VID_TIME',\n",
    "        'rev_per_meal',\n",
    "        'AVG_TIME_PER_SITE_VISIT',\n",
    "        'success_FOLLOWED_RECOMMENDATIONS_PCT',\n",
    "        'FOLLOWED_RECOMMENDATIONS_PCT'\n",
    "    ],\n",
    "    'Gradient Boosting' : [\n",
    "        'CONTACTS_W_CUSTOMER_SERVICE',\n",
    "        'rev_per_pclogin',\n",
    "        'TOTAL_MEALS_ORDERED',\n",
    "        'AVG_PREP_VID_TIME',\n",
    "        'MOBILE_NUMBER',\n",
    "        'rev_per_meal',\n",
    "        'AVG_TIME_PER_SITE_VISIT',\n",
    "        'professional',\n",
    "        'CANCELLATIONS_BEFORE_NOON',\n",
    "        'rev_per_mobilelogin',\n",
    "        'junk',\n",
    "        'FOLLOWED_RECOMMENDATIONS_PCT'\n",
    "    ],\n",
    "    'Best Model' : [\n",
    "        'EARLY_DELIVERIES',\n",
    "          'MOBILE_NUMBER','CANCELLATIONS_BEFORE_NOON',\n",
    "          'CANCELLATIONS_AFTER_NOON','TASTES_AND_PREFERENCES',\n",
    "          'REFRIGERATED_LOCKER','FOLLOWED_RECOMMENDATIONS_PCT',\n",
    "          'personal','professional','junk',\n",
    "          'out_FOLLOWED_RECOMMENDATIONS_PCT_hi',\n",
    "          'out_FOLLOWED_RECOMMENDATIONS_PCT_lo',\n",
    "          'out_PRODUCT_CATEGORIES_VIEWED_hi',\n",
    "          'out_PRODUCT_CATEGORIES_VIEWED_lo',\n",
    "          'out_MEDIAN_MEAL_RATING_hi',\n",
    "          'out_MEDIAN_MEAL_RATING_lo',\n",
    "          'rev_per_mobilelogin',\n",
    "          'out_rev_per_pclogin_hi',\n",
    "          'out_rev_per_pclogin_lo',\n",
    "          'out_rev_per_mobilelogin_hi',\n",
    "          'out_rev_per_mobilelogin_lo',\n",
    "          'success_FOLLOWED_RECOMMENDATIONS_PCT'\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "# Saving model scores\n",
    "# creating an empty list\n",
    "model_performance = [['Model', 'Training Accuracy',\n",
    "                      'Testing Accuracy', 'AUC Value']]\n",
    "\n",
    "# setting random state\n",
    "seed = 222\n",
    "\n",
    "# Defining target variable\n",
    "chef_target = chef.loc[: , variables_dict['target']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with a logistic regression model. We'll be using statsmodel package to print out comprehensive summary to better understand our features and how they relate to cross sell success."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Step 1: </strong> Base Model (original variables) <br>\n",
    "<strong> Step 2: </strong> Full Model (full logical features) <br>\n",
    "<strong> Step 3: </strong> Fitted Model (significant features) <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Non Standardized Preparation \n",
    "## Defining explanatory variables (add according to new feature selections)\n",
    "#chef_base = chef.loc[: , variables_dict['Base']]\n",
    "#\n",
    "## train-test split with stratification\n",
    "#X_train, X_test, y_train, y_test = train_test_split(\n",
    "#            chef_base,  # change\n",
    "#            chef_target,\n",
    "#            test_size = 0.25,\n",
    "#            random_state = seed,\n",
    "#            stratify = chef_target) # stratifying target variable to ensure balance\n",
    "#\n",
    "## merging training data for statsmodels\n",
    "#chef_train = pd.concat([X_train, y_train], axis = 1) # contains target variable!\n",
    "#\n",
    "#\n",
    "####### Standardized Preparation \n",
    "## Standardizing our Data Set (only numeric variables) with user-defined function\n",
    "#chef_stand = standard(chef)\n",
    "#\n",
    "## Defining explanatory variables (add according to new feature selections)\n",
    "#chef_base_stand     = chef_stand.loc[: , variables_dict['Base']]\n",
    "#\n",
    "## train-test split with stratification\n",
    "#X_train_stand, X_test_stand, y_train_stand, y_test_stand = train_test_split(\n",
    "#            chef_base_stand,   # change\n",
    "#            chef_target,\n",
    "#            test_size = 0.25,\n",
    "#            random_state = seed,\n",
    "#            stratify = chef_target) \n",
    "#\n",
    "## merging training data for statsmodels\n",
    "#chef_train_stand = pd.concat([X_train_stand, y_train_stand], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for var in variables_dict['Base']:\n",
    "#    print(f\"{var} + \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## instantiating a logistic regression model object\n",
    "#logistic_base = smf.logit(formula   = \"\"\" CROSS_SELL_SUCCESS ~ \n",
    "#                                          REVENUE + \n",
    "#                                          TOTAL_MEALS_ORDERED + \n",
    "#                                          UNIQUE_MEALS_PURCH + \n",
    "#                                          CONTACTS_W_CUSTOMER_SERVICE + \n",
    "#                                          PRODUCT_CATEGORIES_VIEWED + \n",
    "#                                          AVG_TIME_PER_SITE_VISIT + \n",
    "#                                          MOBILE_NUMBER + \n",
    "#                                          CANCELLATIONS_BEFORE_NOON + \n",
    "#                                          CANCELLATIONS_AFTER_NOON + \n",
    "#                                          TASTES_AND_PREFERENCES + \n",
    "#                                          PC_LOGINS + \n",
    "#                                          MOBILE_LOGINS + \n",
    "#                                          WEEKLY_PLAN + \n",
    "#                                          EARLY_DELIVERIES + \n",
    "#                                          LATE_DELIVERIES + \n",
    "#                                          PACKAGE_LOCKER + \n",
    "#                                          REFRIGERATED_LOCKER + \n",
    "#                                          FOLLOWED_RECOMMENDATIONS_PCT + \n",
    "#                                          AVG_PREP_VID_TIME + \n",
    "#                                          LARGEST_ORDER_SIZE + \n",
    "#                                          MASTER_CLASSES_ATTENDED + \n",
    "#                                          MEDIAN_MEAL_RATING + \n",
    "#                                          AVG_CLICKS_PER_VISIT + \n",
    "#                                          TOTAL_PHOTOS_VIEWED\n",
    "#                                       \"\"\",\n",
    "#                           data = chef_train)\n",
    "#\n",
    "#\n",
    "## FITTING the model object\n",
    "#results_logistic = logistic_base.fit()\n",
    "#\n",
    "#\n",
    "## checking the results SUMMARY\n",
    "##results_logistic.summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Observations: </strong>\n",
    "- LLR p-value: corresponds to the test statiscs for the model. The low p-value indicates that this model is explaining to a certain degree the cross sell success\n",
    "- a lot of statistically insignificant variables are adding noise to our model\n",
    "- significant variables: mobile_number (+0.7131), cancellations before noon (+0.2444), followed recommendations percentage (+0.0572)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Full Model\n",
    "Logistic regression using variables created during exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Non Standardized Preparation \n",
    "## Defining explanatory variables (add according to new feature selections)\n",
    "#chef_full = chef.loc[: , variables_dict['Full Model']]\n",
    "#\n",
    "## train-test split with stratification\n",
    "#X_train, X_test, y_train, y_test = train_test_split(\n",
    "#            chef_full,  # change\n",
    "#            chef_target,\n",
    "#            test_size = 0.25,\n",
    "#            random_state = seed,\n",
    "#            stratify = chef_target) # stratifying target variable to ensure balance\n",
    "#\n",
    "## merging training data for statsmodels\n",
    "#chef_train = pd.concat([X_train, y_train], axis = 1) # contains target variable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for var in variables_dict['Full Model']:\n",
    "#    print(f\"{var} + \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# instantiating a logistic regression model object\n",
    "# removed 'junk' as a base category for emails\n",
    "#logistic_full = smf.logit(formula   = \"\"\" CROSS_SELL_SUCCESS ~ \n",
    "#                                          REVENUE + \n",
    "#                                          TOTAL_MEALS_ORDERED + \n",
    "#                                          UNIQUE_MEALS_PURCH + \n",
    "#                                          CONTACTS_W_CUSTOMER_SERVICE + \n",
    "#                                          PRODUCT_CATEGORIES_VIEWED + \n",
    "#                                          AVG_TIME_PER_SITE_VISIT + \n",
    "#                                          MOBILE_NUMBER + \n",
    "#                                          CANCELLATIONS_BEFORE_NOON + \n",
    "#                                          CANCELLATIONS_AFTER_NOON + \n",
    "#                                          TASTES_AND_PREFERENCES + \n",
    "#                                          PC_LOGINS + \n",
    "#                                          MOBILE_LOGINS + \n",
    "#                                          WEEKLY_PLAN + \n",
    "#                                          EARLY_DELIVERIES + \n",
    "#                                          LATE_DELIVERIES + \n",
    "#                                          PACKAGE_LOCKER + \n",
    "#                                          REFRIGERATED_LOCKER + \n",
    "#                                          FOLLOWED_RECOMMENDATIONS_PCT + \n",
    "#                                          AVG_PREP_VID_TIME + \n",
    "#                                          LARGEST_ORDER_SIZE + \n",
    "#                                          MASTER_CLASSES_ATTENDED + \n",
    "#                                          MEDIAN_MEAL_RATING + \n",
    "#                                          AVG_CLICKS_PER_VISIT + \n",
    "#                                          TOTAL_PHOTOS_VIEWED + \n",
    "#                                          m_FAMILY_NAME + \n",
    "#                                          out_AVG_TIME_PER_SITE_VISIT_hi + \n",
    "#                                          out_AVG_PREP_VID_TIME_hi + \n",
    "#                                          out_TOTAL_MEALS_ORDERED_hi + \n",
    "#                                          out_UNIQUE_MEALS_PURCH_hi + \n",
    "#                                          out_CONTACTS_W_CUSTOMER_SERVICE_hi + \n",
    "#                                          out_CANCELLATIONS_BEFORE_NOON_hi + \n",
    "#                                          out_LATE_DELIVERIES_hi + \n",
    "#                                          out_TOTAL_PHOTOS_VIEWED_hi + \n",
    "#                                          out_REVENUE_hi + \n",
    "#                                          out_FOLLOWED_RECOMMENDATIONS_PCT_hi + \n",
    "#                                          out_LARGEST_ORDER_SIZE_hi + \n",
    "#                                          out_PRODUCT_CATEGORIES_VIEWED_hi + \n",
    "#                                          out_AVG_CLICKS_PER_VISIT_hi + \n",
    "#                                          out_AVG_CLICKS_PER_VISIT_lo + \n",
    "#                                          out_PRODUCT_CATEGORIES_VIEWED_lo + \n",
    "#                                          out_FOLLOWED_RECOMMENDATIONS_PCT_lo + \n",
    "#                                          junk + \n",
    "#                                          personal + \n",
    "#                                          professional + \n",
    "#                                          rev_per_meal + \n",
    "#                                          out_rev_per_meal_hi + \n",
    "#                                          out_rev_per_meal_lo + \n",
    "#                                          success_FOLLOWED_RECOMMENDATIONS_PCT + \n",
    "#                                          success_CANCELLATIONS_BEFORE_NOON + \n",
    "#                                          success_MEDIAN_MEAL_RATING\n",
    "#                                       \"\"\",\n",
    "#                           data = chef_train)\n",
    "#\n",
    "#\n",
    "## FITTING the model object\n",
    "#results_logistic = logistic_full.fit()\n",
    "#\n",
    "#\n",
    "## checking the results SUMMARY\n",
    "##results_logistic.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Observations: </strong>\n",
    "- Significant Variables:\n",
    "    - mobile number\n",
    "    - cancellations before nooon\n",
    "    - followed recommendations percentage\n",
    "    - outliers in followed recommendations percentage lo\n",
    "    - outlier in rev per meal lo\n",
    "    - personal and professional emails (opposed to junk)\n",
    "    - success in followed recommendations percentage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Fitted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating a logistic regression model object\n",
    "# removed 'junk' as a base category for emails\n",
    "#logistic_fit = smf.logit(formula   = \"\"\" CROSS_SELL_SUCCESS ~ \n",
    "#                                          MOBILE_NUMBER + \n",
    "#                                          MOBILE_LOGINS +\n",
    "#                                          FOLLOWED_RECOMMENDATIONS_PCT +\n",
    "#                                          CANCELLATIONS_BEFORE_NOON + \n",
    "#                                          TASTES_AND_PREFERENCES +\n",
    "#                                          AVG_PREP_VID_TIME +\n",
    "#                                          out_FOLLOWED_RECOMMENDATIONS_PCT_lo + \n",
    "#                                          out_rev_per_meal_lo +\n",
    "#                                          personal + \n",
    "#                                          professional +\n",
    "#                                          success_FOLLOWED_RECOMMENDATIONS_PCT \"\"\",\n",
    "#                           data = chef_train)\n",
    "#\n",
    "#\n",
    "## FITTING the model object\n",
    "#results_logistic = logistic_fit.fit()\n",
    "##results_logistic.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Observations: </strong>\n",
    "- tastes and preferences seem to not be as significant but since it is an actionable variable, we'll keep it in the model for further analysis\n",
    "    - tastes and preference is at the moment an optional step for customers in the registration process\n",
    "    - it might be that customers that fill out the tastes and preferences are more likely to be engaged with the service and more likely to encounter the promotion, or more likely to be interested in improving their experience\n",
    "    - tastes and preferences could include a section on types of wine the customer enjoys\n",
    "    \n",
    "- since mobile number is significant, added mobile logins to understand customer purchase behavior\n",
    "- average prep video time: somewhat significant, increases significance when we include other variables related to time on platform (avg clicks per visit, total photos viewed)\n",
    "    - might indicate that perhaps customers enjoy having a glass of wine while preparing their food\n",
    "    - marketing opportunity: add promotion on these videos (in house campaign has benefits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Important Model Scores (sklearn)\n",
    "Logistic regression using sklearn to find more details on the scoring. Still using the variables that showed important through exploratory data analysis:\n",
    "- Mobile Number\n",
    "- Mobile Logins\n",
    "- AVG prep video time\n",
    "- Cancellations Before Noon\n",
    "- Tastes and Preferences\n",
    "- Followed Recommendation Percentages (+ outlier and success flag)\n",
    "- email categories: junk ,professional, personal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.7629\n",
      "Testing  ACCURACY: 0.7392\n",
      "AUC Score        : 0.6997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "###### Non Standardized Preparation \n",
    "# Defining explanatory variables (add according to new feature selections)\n",
    "chef_imp = chef.loc[: , variables_dict['Best Model']]\n",
    "\n",
    "# train-test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "            chef_imp,  # change\n",
    "            chef_target,\n",
    "            test_size = 0.25,\n",
    "            random_state = seed,\n",
    "            stratify = chef_target) # stratifying target variable to ensure balance\n",
    "\n",
    "# merging training data for statsmodels\n",
    "chef_train = pd.concat([X_train, y_train], axis = 1) # contains target variable!\n",
    "\n",
    "\n",
    "# INSTANTIATING a logistic regression model\n",
    "logreg = LogisticRegression(random_state = seed)\n",
    "\n",
    "# FITTING the training data\n",
    "logreg_fit = logreg.fit(X_train, y_train.values.reshape(-1,))\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "logreg_pred = logreg_fit.predict(X_test)\n",
    "\n",
    "# train accuracy\n",
    "logreg_train_acc  = logreg_fit.score(X_train, y_train).round(4)\n",
    "\n",
    "# test accuracy\n",
    "logreg_test_acc   = logreg_fit.score(X_test, y_test).round(4)\n",
    "\n",
    "# auc value\n",
    "logreg_auc = roc_auc_score(y_true  = y_test,\n",
    "                           y_score = logreg_pred).round(4)\n",
    "\n",
    "print('Training ACCURACY:', logreg_train_acc)\n",
    "print('Testing  ACCURACY:', logreg_test_acc)\n",
    "print('AUC Score        :', logreg_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the results\n",
    "model_performance.append(['Logistic Regression',\n",
    "                          logreg_train_acc,\n",
    "                          logreg_test_acc,\n",
    "                          logreg_auc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Observations: </strong>\n",
    "- model is well fitted based on training and testing score\n",
    "- Convergence warning: need to increase number of iterations or run on scaled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Logistic on Scaled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.782\n",
      "Testing  ACCURACY: 0.7988\n",
      "AUC Score        : 0.7655\n"
     ]
    }
   ],
   "source": [
    "###### Standardized Preparation \n",
    "# Standardizing our Data Set (only numeric variables) with user-defined function\n",
    "chef_stand = standard(chef)\n",
    "\n",
    "# Defining explanatory variables (add according to new feature selections)\n",
    "chef_imp_stand     = chef_stand.loc[: , variables_dict['Best Model']]\n",
    "\n",
    "# train-test split with stratification\n",
    "X_train_stand, X_test_stand, y_train_stand, y_test_stand = train_test_split(\n",
    "            chef_imp_stand,   # change\n",
    "            chef_target,\n",
    "            test_size = 0.25,\n",
    "            random_state = seed,\n",
    "            stratify = chef_target) \n",
    "\n",
    "# merging training data for statsmodels\n",
    "chef_train_stand = pd.concat([X_train_stand, y_train_stand], axis = 1)\n",
    "\n",
    "# Important model on Standardized data\n",
    "# INSTANTIATING a logistic regression model\n",
    "logreg_stand = LogisticRegression(random_state = seed) \n",
    "\n",
    "# FITTING the training data\n",
    "logreg_fit_stand = logreg_stand.fit(X_train_stand, y_train.values.reshape(-1,)) # removes warning on column shape\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "logreg_pred_stand = logreg_fit_stand.predict(X_test_stand)\n",
    "\n",
    "# train accuracy\n",
    "logreg_train_acc_stand  = logreg_fit_stand.score(X_train_stand, y_train_stand).round(4)\n",
    "\n",
    "# test accuracy\n",
    "logreg_test_acc_stand   = logreg_fit_stand.score(X_test_stand, y_test_stand).round(4)\n",
    "\n",
    "# auc value\n",
    "logreg_auc_stand = roc_auc_score(y_true  = y_test_stand,\n",
    "                           y_score = logreg_pred_stand).round(4)\n",
    "\n",
    "print('Training ACCURACY:', logreg_train_acc_stand)\n",
    "print('Testing  ACCURACY:', logreg_test_acc_stand)\n",
    "print('AUC Score        :', logreg_auc_stand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the results\n",
    "model_performance.append(['Logistic Regression - Standardized',\n",
    "                          logreg_train_acc_stand,\n",
    "                          logreg_test_acc_stand,\n",
    "                          logreg_auc_stand])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Observations: </strong>\n",
    "- model on standardized data slightly improved in scores\n",
    "- run hyperparameter tuning to increase score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "### GridSearchCV\n",
    "##########################################\n",
    "## declaring a hyperparameter space\n",
    "#solver_space     = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'] # use L2 penalty\n",
    "#C_space          = pd.np.arange(0.1, 3.0, 0.1)\n",
    "#warm_start_space = [True, False]\n",
    "#\n",
    "#\n",
    "## creating a hyperparameter grid\n",
    "#param_grid = {'C'            : C_space,          \n",
    "#              'warm_start'   : warm_start_space,\n",
    "#              'solver'       : solver_space\n",
    "#             } \n",
    "#\n",
    "#\n",
    "## INSTANTIATING the model object without hyperparameters\n",
    "#lr_tuned = LogisticRegression(max_iter = 1000,\n",
    "#                              random_state = seed)\n",
    "#\n",
    "#\n",
    "## GridSearchCV object\n",
    "#lr_tuned_cv = GridSearchCV(estimator  = lr_tuned, \n",
    "#                           param_grid = param_grid, \n",
    "#                           cv         = 3, \n",
    "#                           scoring    = make_scorer(roc_auc_score,\n",
    "#                                                    needs_threshold = False))\n",
    "#\n",
    "#\n",
    "## FITTING to the FULL DATASET (due to cross-validation)\n",
    "#lr_tuned_cv.fit(chef_imp_stand, chef_target.values.reshape(-1,))\n",
    "#\n",
    "#\n",
    "## printing the optimal parameters and best score\n",
    "#print(\"Tuned Parameters  :\", lr_tuned_cv.best_params_)\n",
    "#print(\"Tuned CV AUC      :\", lr_tuned_cv.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearch Output: <br>\n",
    "Tuned Parameters  : {'C': 1.6, 'solver': 'newton-cg', 'warm_start': True} <br>\n",
    "Tuned CV AUC      : 0.6117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.78\n",
      "Testing  ACCURACY: 0.7988\n",
      "AUC Score        : 0.7655\n"
     ]
    }
   ],
   "source": [
    "# Model with Tuned Parameters\n",
    "# INSTANTIATING a logistic regression model\n",
    "logreg_tuned = LogisticRegression(solver = 'newton-cg',\n",
    "                                  C = 1.6,\n",
    "                                  max_iter = 1500,\n",
    "                                  warm_start = True) \n",
    "\n",
    "# FITTING the training data\n",
    "logreg_fit_tuned = logreg_tuned.fit(X_train_stand, y_train_stand.values.reshape(-1,)) # removes warning on column shap\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "logreg_pred_tuned = logreg_fit_tuned.predict(X_test_stand)\n",
    "\n",
    "# train accuracy\n",
    "logreg_train_acc_tuned  = logreg_fit_tuned.score(X_train_stand, y_train_stand).round(4)\n",
    "\n",
    "# test accuracy\n",
    "logreg_test_acc_tuned   = logreg_fit_tuned.score(X_test_stand, y_test_stand).round(4)\n",
    "\n",
    "# auc value\n",
    "logreg_auc_tuned = roc_auc_score(y_true  = y_test_stand,\n",
    "                                y_score = logreg_pred_tuned).round(4)\n",
    "\n",
    "print('Training ACCURACY:', logreg_train_acc_tuned)\n",
    "print('Testing  ACCURACY:', logreg_test_acc_tuned)\n",
    "print('AUC Score        :', logreg_auc_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the results\n",
    "model_performance.append(['Logistic Regression - Tuned',\n",
    "                          logreg_train_acc_tuned,\n",
    "                          logreg_test_acc_tuned,\n",
    "                          logreg_auc_tuned])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) K-Nearest Neighbors Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Standardized Preparation \n",
    "# Standardizing our Data Set (only numeric variables) with user-defined function\n",
    "chef_stand = standard(chef)\n",
    "\n",
    "# Defining explanatory variables (add according to new feature selections)\n",
    "chef_full_stand   = chef_stand.loc[: , variables_dict['Full Model']]\n",
    "\n",
    "# train-test split with stratification\n",
    "X_train_stand, X_test_stand, y_train_stand, y_test_stand = train_test_split(\n",
    "            chef_full_stand,   # change\n",
    "            chef_target,\n",
    "            test_size = 0.25,\n",
    "            random_state = seed,\n",
    "            stratify = chef_target) \n",
    "\n",
    "# merging training data for statsmodels\n",
    "chef_train_stand = pd.concat([X_train_stand, y_train_stand], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.8225\n",
      "Testing  ACCURACY: 0.6509\n",
      "AUC Score        : 0.6754\n"
     ]
    }
   ],
   "source": [
    "# finding optimal number of neighbors using AUC Score\n",
    "opt_neighbors = optimal_neighbors('auc', X_train, y_train, X_test, y_test, 20)\n",
    "\n",
    "# Only use standatdized data\n",
    "# INSTANTIATING a KNN model object\n",
    "full_knn_class = KNeighborsClassifier(n_neighbors = opt_neighbors)\n",
    "\n",
    "# FITTING to the training data\n",
    "full_knn_class_fit_stand = full_knn_class.fit(X_train_stand, y_train_stand.values.reshape(-1,))\n",
    "\n",
    "# PREDICTING on new data\n",
    "full_knn_class_pred = full_knn_class.predict(X_test_stand)\n",
    "\n",
    "# train accuracy\n",
    "full_knn_class_train_acc  = full_knn_class_fit_stand.score(X_train_stand, y_train_stand).round(4)\n",
    "\n",
    "# test accuracy\n",
    "full_knn_class_test_acc = full_knn_class_fit_stand.score(X_test_stand, y_test_stand).round(4)\n",
    "\n",
    "# auc value\n",
    "full_knn_class_auc = roc_auc_score(y_true  = y_test_stand,\n",
    "                              y_score = full_knn_class_pred).round(4)\n",
    "\n",
    "print('Training ACCURACY:', full_knn_class_train_acc)\n",
    "print('Testing  ACCURACY:', full_knn_class_test_acc)\n",
    "print('AUC Score        :', full_knn_class_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the results\n",
    "model_performance.append(['Full KNN',\n",
    "                          full_knn_class_train_acc,\n",
    "                          full_knn_class_test_acc,\n",
    "                          full_knn_class_auc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Observations: </strong>\n",
    "- model is overfit but does performs better than logistic regression\n",
    "- use KNN with important features next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Model with important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Standardized Preparation \n",
    "## Standardizing our Data Set (only numeric variables) with user-defined function\n",
    "#chef_stand = standard(chef)\n",
    "#\n",
    "## Defining explanatory variables (add according to new feature selections)\n",
    "#chef_imp_stand   = chef_stand.loc[: , variables_dict['Best Model']]\n",
    "#\n",
    "## train-test split with stratification\n",
    "#X_train_stand, X_test_stand, y_train_stand, y_test_stand = train_test_split(\n",
    "#            chef_imp_stand,   # change\n",
    "#            chef_target,\n",
    "#            test_size = 0.25,\n",
    "#            random_state = seed,\n",
    "#            stratify = chef_target) \n",
    "#\n",
    "## merging training data for statsmodels\n",
    "#chef_train_stand = pd.concat([X_train_stand, y_train_stand], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## finding optimal number of neighbors using AUC Score\n",
    "#opt_neighbors = optimal_neighbors('auc', X_train, y_train, X_test, y_test, 20)\n",
    "#\n",
    "## Only use standatdized data\n",
    "## INSTANTIATING a KNN model object\n",
    "#knn_class = KNeighborsClassifier(n_neighbors = opt_neighbors)\n",
    "#\n",
    "## FITTING to the training data\n",
    "#knn_class_fit_stand = knn_class.fit(X_train_stand, y_train_stand.values.reshape(-1,))\n",
    "#\n",
    "## PREDICTING on new data\n",
    "#knn_class_pred = knn_class.predict(X_test_stand)\n",
    "#\n",
    "## train accuracy\n",
    "#knn_class_train_acc  = knn_class_fit_stand.score(X_train_stand, y_train_stand).round(4)\n",
    "#\n",
    "## test accuracy\n",
    "#knn_class_test_acc = knn_class_fit_stand.score(X_test_stand, y_test_stand).round(4)\n",
    "#\n",
    "## auc value\n",
    "#knn_class_auc = roc_auc_score(y_true  = y_test_stand,\n",
    "#                              y_score = knn_class_pred).round(4)\n",
    "#\n",
    "#print('Training ACCURACY:', knn_class_train_acc)\n",
    "#print('Testing  ACCURACY:', knn_class_test_acc)\n",
    "#print('AUC Score        :', knn_class_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## saving the results\n",
    "#model_performance.append(['KNN',\n",
    "#                          knn_class_train_acc,\n",
    "#                          knn_class_test_acc,\n",
    "#                          knn_class_auc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Observations: </strong>\n",
    "- reduced model performance with KNN\n",
    "- model is still fitting well\n",
    "- can improve with hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "### GridSearchCV\n",
    "##########################################\n",
    "## declaring a hyperparameter space\n",
    "#n_neighbors_space = pd.np.arange(1,20,1)    \n",
    "#weights_space     = ['uniform','distance']\n",
    "#leaf_size_space   = pd.np.arange(10,30,10)\n",
    "#\n",
    "#\n",
    "## creating a hyperparameter grid\n",
    "#param_grid = {'n_neighbors'  : n_neighbors_space,          \n",
    "#              'weights'      : weights_space,\n",
    "#              'leaf_size'    : leaf_size_space\n",
    "#             } \n",
    "#\n",
    "#\n",
    "## INSTANTIATING the model object without hyperparameters\n",
    "#knn_tuned = KNeighborsClassifier()\n",
    "#\n",
    "#\n",
    "## GridSearchCV object\n",
    "#knn_tuned_cv = GridSearchCV(estimator  = knn_tuned, \n",
    "#                           param_grid = param_grid, \n",
    "#                           cv         = 3, \n",
    "#                           scoring    = make_scorer(roc_auc_score,\n",
    "#                                                    needs_threshold = False))\n",
    "#\n",
    "#\n",
    "## FITTING to the FULL DATASET (due to cross-validation)\n",
    "#knn_tuned_cv.fit(chef_imp_stand, chef_target.values.reshape(-1,))\n",
    "#\n",
    "#\n",
    "## PREDICT step is not needed\n",
    "#\n",
    "#\n",
    "## printing the optimal parameters and best score\n",
    "#print(\"Tuned Parameters  :\", knn_tuned_cv.best_params_)\n",
    "#print(\"Tuned CV AUC      :\", knn_tuned_cv.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearch Output: <br>\n",
    "Tuned Parameters  : {{'leaf_size': 10, 'n_neighbors': 2, 'weights': 'uniform'} <br>\n",
    "Tuned CV AUC      : 0.6372"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.8225\n",
      "Testing  ACCURACY: 0.6509\n",
      "AUC Score        : 0.6754\n"
     ]
    }
   ],
   "source": [
    "# Running KNN on tuned parameters\n",
    "# INSTANTIATING a KNN model object\n",
    "knn_class_tuned = KNeighborsClassifier(leaf_size   = 10, \n",
    "                                       n_neighbors = 2,\n",
    "                                       weights     = 'uniform')\n",
    "\n",
    "# FITTING to the training data\n",
    "knn_class_fit_tuned = knn_class_tuned.fit(X_train_stand, y_train_stand.values.reshape(-1,))\n",
    "\n",
    "# PREDICTING on new data\n",
    "knn_class_pred_tuned = knn_class_tuned.predict(X_test_stand)\n",
    "\n",
    "# train accuracy\n",
    "knn_class_train_acc_tuned  = knn_class_fit_tuned.score(X_train_stand, y_train_stand).round(4)\n",
    "\n",
    "# test accuracy\n",
    "knn_class_test_acc_tuned = knn_class_fit_tuned.score(X_test_stand, y_test_stand).round(4)\n",
    "\n",
    "# auc value\n",
    "knn_class_auc_tuned = roc_auc_score(y_true  = y_test_stand,\n",
    "                                    y_score = knn_class_pred_tuned).round(4)\n",
    "\n",
    "print('Training ACCURACY:', knn_class_train_acc_tuned)\n",
    "print('Testing  ACCURACY:', knn_class_test_acc_tuned)\n",
    "print('AUC Score        :', knn_class_auc_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the results\n",
    "model_performance.append(['KNN Tuned',\n",
    "                          knn_class_train_acc_tuned,\n",
    "                          knn_class_test_acc_tuned,\n",
    "                          knn_class_auc_tuned])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Observations: </strong>\n",
    "- Tuned parameters do not increase AUC score\n",
    "- keep default parameters with optimal number of neighbors with Best Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) CART Model (Decision Tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Default Model with all Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####### Non Standardized Preparation \n",
    "## Defining explanatory variables (add according to new feature selections)\n",
    "#chef_full = chef.loc[: , variables_dict['Full Model']]\n",
    "#\n",
    "## train-test split with stratification\n",
    "#X_train, X_test, y_train, y_test = train_test_split(\n",
    "#            chef_full,  # change\n",
    "#            chef_target,\n",
    "#            test_size = 0.25,\n",
    "#            random_state = seed,\n",
    "#            stratify = chef_target) # stratifying target variable to ensure balance\n",
    "#\n",
    "## merging training data for statsmodels\n",
    "#chef_train = pd.concat([X_train, y_train], axis = 1) # contains target variable!\n",
    "#\n",
    "## INSTANTIATING a classification tree object\n",
    "#full_tree = DecisionTreeClassifier(random_state = seed)\n",
    "#\n",
    "## FITTING the training data\n",
    "#full_tree_fit = full_tree.fit(X_train, y_train.values.reshape(-1,))\n",
    "#\n",
    "## PREDICTING on new data\n",
    "#full_tree_pred = full_tree_fit.predict(X_test)\n",
    "#\n",
    "#full_tree_train_acc = full_tree_fit.score(X_train, y_train).round(4)\n",
    "#\n",
    "#full_tree_test_acc = full_tree_fit.score(X_test, y_test).round(4)\n",
    "#\n",
    "#full_tree_auc = roc_auc_score(y_true  = y_test, y_score = full_tree_pred).round(4)\n",
    "#\n",
    "## SCORING the model\n",
    "#print('Training ACCURACY:', full_tree_fit.score(X_train, y_train).round(4))\n",
    "#print('Testing  ACCURACY:', full_tree_fit.score(X_test, y_test).round(4))\n",
    "#print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "#                                          y_score = full_tree_pred).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_performance.append(['Full Tree',\n",
    "#                          full_tree_train_acc,\n",
    "#                          full_tree_test_acc,\n",
    "#                          full_tree_auc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tunining tree parameters to fit best variable set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Non Standardized Preparation \n",
    "# Defining explanatory variables (add according to new feature selections)\n",
    "chef_best = chef.loc[: , variables_dict['Best Model']]\n",
    "\n",
    "# train-test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "            chef_best,  # change\n",
    "            chef_target,\n",
    "            test_size = 0.25,\n",
    "            random_state = seed,\n",
    "            stratify = chef_target) # stratifying target variable to ensure balance\n",
    "\n",
    "# merging training data for statsmodels\n",
    "chef_train = pd.concat([X_train, y_train], axis = 1) # contains target variable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.9075\n",
      "Testing  ACCURACY: 0.8973\n",
      "AUC Score        : 0.9058\n"
     ]
    }
   ],
   "source": [
    "## declaring a hyperparameter space\n",
    "#criterion_space = ['gini', 'entropy']\n",
    "#splitter_space = ['best', 'random']\n",
    "#depth_space   = pd.np.arange(1, 25)\n",
    "#leaf_space    = pd.np.arange(1, 100)\n",
    "#\n",
    "#\n",
    "## creating a hyperparameter grid\n",
    "#param_grid = {'criterion'        : criterion_space,\n",
    "#              'splitter'         : splitter_space,\n",
    "#              'max_depth'        : depth_space,\n",
    "#              'min_samples_leaf' : leaf_space}\n",
    "#\n",
    "#\n",
    "## INSTANTIATING the model object without hyperparameters\n",
    "#tuned_tree = DecisionTreeClassifier(random_state = seed)\n",
    "#\n",
    "#\n",
    "## GridSearchCV object\n",
    "#tuned_tree_cv = GridSearchCV(estimator  = tuned_tree,\n",
    "#                             param_grid = param_grid,\n",
    "#                             cv         = 3,\n",
    "#                             scoring    = make_scorer(roc_auc_score,\n",
    "#                                                      needs_threshold = False))\n",
    "#\n",
    "#\n",
    "# INSTANTIATING a logistic regression model with tuned values\n",
    "tree_tuned = DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
    "                                    max_depth=18, max_features=None, max_leaf_nodes=None,\n",
    "                                    min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                                    min_samples_leaf=2, min_samples_split=2,\n",
    "                                    min_weight_fraction_leaf=0.0, presort='deprecated',\n",
    "                                    random_state=seed, splitter='random')\n",
    "\n",
    "\n",
    "# FITTING to the FULL DATASET (due to cross-validation from GridSearch)\n",
    "tree_tuned_fit = tree_tuned.fit(chef_best, chef_target)\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "tree_tuned_pred = tree_tuned.predict(X_test)\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', tree_tuned.score(X_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', tree_tuned.score(X_test, y_test).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = tree_tuned_pred).round(4))\n",
    "\n",
    "# declaring model performance objects\n",
    "tree_train_acc = tree_tuned.score(X_train, y_train).round(4)\n",
    "tree_test_acc  = tree_tuned.score(X_test, y_test).round(4)\n",
    "tree_auc       = roc_auc_score(y_true  = y_test,\n",
    "                               y_score = tree_tuned_pred).round(4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> BEST MODEL </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending to model_performance\n",
    "model_performance.append(['Tuned Tree',\n",
    "                          tree_train_acc,\n",
    "                          tree_test_acc,\n",
    "                          tree_auc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "## storing important features from gradient boosting as df \n",
    "tree_features = pd.DataFrame(tree_tuned_fit.feature_importances_)\n",
    "tree_features['Variable Names'] = chef_best.columns\n",
    "\n",
    "# Looking at insignificant coefficients\n",
    "tree_features = tree_features.iloc[:,:][tree_features[0] != 0].sort_values(by = 0) # cut-off based on boxplots analysis\n",
    "#tree_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Observations: </strong>\n",
    "- Tree model identified additional features of importance to our case (compared to logistic regerssion): \n",
    "    - average clicks per visit\n",
    "    - weekly plan\n",
    "    - revenue\n",
    "    - total photos viewed\n",
    "    - average prep video time\n",
    "    \n",
    "- variables that were important in logisitic that were not in this model:\n",
    "    - mobile number\n",
    "    - tastes and preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.8095\n",
      "Testing  ACCURACY: 0.7864\n",
      "AUC Score        : 0.7836\n"
     ]
    }
   ],
   "source": [
    "###### Non Standardized Preparation \n",
    "# Defining explanatory variables (add according to new feature selections)\n",
    "chef_tree = chef.loc[: , variables_dict['Full Tree Features']]\n",
    "\n",
    "# train-test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "            chef_tree,  # change\n",
    "            chef_target,\n",
    "            test_size = 0.25,\n",
    "            random_state = seed,\n",
    "            stratify = chef_target) # stratifying target variable to ensure balance\n",
    "\n",
    "# merging training data for statsmodels\n",
    "chef_train = pd.concat([X_train, y_train], axis = 1) # contains target variable!\n",
    "\n",
    "# INSTANTIATING a classification tree object\n",
    "fit_tree = DecisionTreeClassifier(criterion = 'gini',\n",
    "                                   max_depth = 5,\n",
    "                                   min_samples_leaf = 59,\n",
    "                                   splitter = 'best',\n",
    "                                   random_state = seed)\n",
    "\n",
    "# FITTING the training data\n",
    "fit_tree_fit = fit_tree.fit(X_train, y_train.values.reshape(-1,))\n",
    "\n",
    "# PREDICTING on new data\n",
    "fit_tree_pred = fit_tree_fit.predict(X_test)\n",
    "\n",
    "fit_tree_train_acc = fit_tree_fit.score(X_train, y_train).round(4)\n",
    "\n",
    "fit_tree_test_acc = fit_tree_fit.score(X_test, y_test).round(4)\n",
    "\n",
    "fit_tree_auc = roc_auc_score(y_true  = y_test, y_score = fit_tree_pred).round(4)\n",
    "\n",
    "# SCORING the model\n",
    "print('Training ACCURACY:', fit_tree_train_acc)\n",
    "print('Testing  ACCURACY:', fit_tree_test_acc)\n",
    "print('AUC Score        :', fit_tree_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performance.append(['Fit Tree',\n",
    "                          fit_tree_train_acc,\n",
    "                          fit_tree_test_acc,\n",
    "                          fit_tree_auc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D) Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Default Model with all Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 1.0\n",
      "Testing  ACCURACY: 0.7454\n",
      "AUC Score        : 0.7025\n"
     ]
    }
   ],
   "source": [
    "###### Non Standardized Preparation \n",
    "# Defining explanatory variables (add according to new feature selections)\n",
    "chef_full = chef.loc[: , variables_dict['Full Model']]\n",
    "\n",
    "# train-test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "            chef_full,  # change\n",
    "            chef_target,\n",
    "            test_size = 0.25,\n",
    "            random_state = seed,\n",
    "            stratify = chef_target) # stratifying target variable to ensure balance\n",
    "\n",
    "# merging training data for statsmodels\n",
    "chef_train = pd.concat([X_train, y_train], axis = 1) # contains target variable!\n",
    "\n",
    "# INSTANTIATING a random forest model with default values\n",
    "rf_default = RandomForestClassifier(random_state  = seed)\n",
    "\n",
    "# FITTING the training data\n",
    "rf_default_fit = rf_default.fit(X_train, y_train.values.reshape(-1,))\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "rf_default_fit_pred = rf_default_fit.predict(X_test)\n",
    "\n",
    "rf_train_acc = rf_default_fit.score(X_train, y_train).round(4)\n",
    "rf_test_acc  = rf_default_fit.score(X_test, y_test).round(4)\n",
    "rf_auc_score = roc_auc_score(y_true  = y_test,\n",
    "                             y_score = rf_default_fit_pred).round(4)\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', rf_train_acc)\n",
    "print('Testing  ACCURACY:', rf_test_acc)\n",
    "print('AUC Score        :', rf_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the results\n",
    "model_performance.append(['Random Forest Default Full',\n",
    "                          rf_train_acc,\n",
    "                          rf_test_acc,\n",
    "                          rf_auc_score])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Random Forest Hyperparameter Tuning\n",
    "## declaring a hyperparameter space\n",
    "#estimator_space  = pd.np.arange(100, 1100, 250)\n",
    "#leaf_space       = pd.np.arange(1, 31, 10)\n",
    "#criterion_space  = ['gini', 'entropy']\n",
    "#bootstrap_space  = [True, False]\n",
    "#warm_start_space = [True, False]\n",
    "#\n",
    "#\n",
    "## creating a hyperparameter grid\n",
    "#param_grid = {'n_estimators'     : estimator_space,\n",
    "#              'min_samples_leaf' : leaf_space,\n",
    "#              'criterion'        : criterion_space,\n",
    "#              'bootstrap'        : bootstrap_space,\n",
    "#              'warm_start'       : warm_start_space}\n",
    "#\n",
    "#\n",
    "## INSTANTIATING the model object without hyperparameters\n",
    "#full_forest_grid = RandomForestClassifier(random_state = 802)\n",
    "#\n",
    "#\n",
    "## GridSearchCV object\n",
    "#full_forest_cv = GridSearchCV(estimator  = full_forest_grid,\n",
    "#                              param_grid = param_grid,\n",
    "#                              cv         = 3,\n",
    "#                              scoring    = make_scorer(roc_auc_score,\n",
    "#                                           needs_threshold = False))\n",
    "#\n",
    "#\n",
    "## FITTING to the FULL DATASET (due to cross-validation)\n",
    "#full_forest_cv.fit(chef_full, chef_target.values.reshape(-1,))\n",
    "#\n",
    "#\n",
    "## PREDICT step is not needed\n",
    "#\n",
    "#\n",
    "## printing the optimal parameters and best score\n",
    "#print(\"Tuned Parameters  :\", full_forest_cv.best_params_)\n",
    "#print(\"Tuned Training AUC:\", full_forest_cv.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Full set </strong>:\n",
    "Tuned Parameters  : {'bootstrap': False, 'criterion': 'entropy', 'min_samples_leaf': 1, 'n_estimators': 600, 'warm_start': True}\n",
    "Tuned Training AUC: 0.5882\n",
    "<br>\n",
    "Tree set: \n",
    "Tuned Parameters  : {'bootstrap': False, 'criterion': 'entropy', 'min_samples_leaf': 1, 'n_estimators': 350, 'warm_start': True} <br>\n",
    "Tuned Training AUC: 0.5836"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 1.0\n",
      "Testing  ACCURACY: 0.7598\n",
      "AUC Score        : 0.725\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING a random forest model with tuned parameters\n",
    "rf_tuned = RandomForestClassifier(bootstrap = False, \n",
    "                                    criterion = 'entropy',\n",
    "                                    min_samples_leaf = 1,\n",
    "                                    n_estimators = 350,\n",
    "                                    warm_start = True,\n",
    "                                    random_state  = seed)\n",
    "\n",
    "# FITTING the training data\n",
    "rf_tuned_fit = rf_tuned.fit(X_train, y_train.values.reshape(-1,))\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "rf_tuned_fit_pred = rf_tuned_fit.predict(X_test)\n",
    "\n",
    "rf_train_acc_tuned = rf_tuned_fit.score(X_train, y_train).round(4)\n",
    "rf_test_acc_tuned  = rf_tuned_fit.score(X_test, y_test).round(4)\n",
    "rf_auc_score_tuned = roc_auc_score(y_true  = y_test,\n",
    "                                   y_score = rf_tuned_fit_pred).round(4)\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', rf_train_acc_tuned)\n",
    "print('Testing  ACCURACY:', rf_test_acc_tuned)\n",
    "print('AUC Score        :', rf_auc_score_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the results\n",
    "model_performance.append(['Random Forest Tuned Full',\n",
    "                          rf_train_acc_tuned,\n",
    "                          rf_test_acc_tuned,\n",
    "                          rf_auc_score_tuned])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## storing important features from random forest as df \n",
    "rf_features = pd.DataFrame(rf_default_fit.feature_importances_)\n",
    "rf_features['Variable Names'] = chef_full.columns\n",
    "\n",
    "# Looking at insignificant coefficients\n",
    "rf_features = rf_features.iloc[:,:][rf_features[0] >= 0.03].sort_values(by = 0) # sorting for most important features\n",
    "#rf_features.sort_values(by = 0, ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Observations: </strong>\n",
    "- Tree identified rev_per_meal as a successful variable which was not selected before\n",
    "- unique meals purchased and product categories viewed were also important here but not previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 1.0\n",
      "Testing  ACCURACY: 0.7659\n",
      "AUC Score        : 0.738\n"
     ]
    }
   ],
   "source": [
    "###### Non Standardized Preparation \n",
    "# Defining explanatory variables (add according to new feature selections)\n",
    "chef_rf = chef.loc[: , variables_dict['Random Forest Full']]\n",
    "\n",
    "# train-test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "            chef_rf,  # change\n",
    "            chef_target,\n",
    "            test_size = 0.25,\n",
    "            random_state = seed,\n",
    "            stratify = chef_target) # stratifying target variable to ensure balance\n",
    "\n",
    "# merging training data for statsmodels\n",
    "chef_train = pd.concat([X_train, y_train], axis = 1) # contains target variable!\n",
    "\n",
    "# INSTANTIATING a random forest model\n",
    "rf = RandomForestClassifier(bootstrap = False, \n",
    "                            criterion = 'entropy',\n",
    "                            min_samples_leaf = 1,\n",
    "                            n_estimators = 350,\n",
    "                            warm_start = True,\n",
    "                            random_state  = seed)\n",
    "\n",
    "# FITTING the training data\n",
    "rf_fit = rf.fit(X_train, y_train.values.reshape(-1,))\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "rf_pred = rf_fit.predict(X_test)\n",
    "\n",
    "rf_train_acc2 = rf_fit.score(X_train, y_train).round(4)\n",
    "rf_test_acc2  = rf_fit.score(X_test, y_test).round(4)\n",
    "rf_auc_score2 = roc_auc_score(y_true  = y_test,\n",
    "                             y_score = rf_pred).round(4)\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', rf_train_acc2)\n",
    "print('Testing  ACCURACY:', rf_test_acc2)\n",
    "print('AUC Score        :', rf_auc_score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the results\n",
    "model_performance.append(['Random Forest Tuned Fit',\n",
    "                          rf_train_acc2,\n",
    "                          rf_test_acc2,\n",
    "                          rf_auc_score2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E) Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Default Model with all Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.9034\n",
      "Testing ACCURACY : 0.7659\n",
      "AUC Score        : 0.7431\n"
     ]
    }
   ],
   "source": [
    "###### Standardized Preparation \n",
    "# Standardizing our Data Set (only numeric variables) with user-defined function\n",
    "chef_stand = standard(chef)\n",
    "\n",
    "# Defining explanatory variables (add according to new feature selections)\n",
    "chef_full_stand   = chef_stand.loc[: , variables_dict['Full Model']]\n",
    "\n",
    "# train-test split with stratification\n",
    "X_train_stand, X_test_stand, y_train_stand, y_test_stand = train_test_split(\n",
    "            chef_full_stand,   # change\n",
    "            chef_target,\n",
    "            test_size = 0.25,\n",
    "            random_state = seed,\n",
    "            stratify = chef_target) \n",
    "\n",
    "# merging training data for statsmodels\n",
    "chef_train_stand = pd.concat([X_train_stand, y_train_stand], axis = 1)\n",
    "\n",
    "\n",
    "# INSTANTIATING the model object without hyperparameters\n",
    "full_gbm_default = GradientBoostingClassifier()\n",
    "\n",
    "\n",
    "# FIT step is needed as we are not using .best_estimator\n",
    "full_gbm_default_fit = full_gbm_default.fit(X_train_stand, y_train_stand.values.reshape(-1,))\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "full_gbm_default_pred = full_gbm_default_fit.predict(X_test_stand)\n",
    "\n",
    "gbm_train_score = full_gbm_default_fit.score(X_train_stand, y_train_stand).round(4)\n",
    "gbm_test_score  = full_gbm_default_fit.score(X_test_stand, y_test_stand).round(4)\n",
    "gbm_auc_score   = roc_auc_score(y_true  = y_test_stand,\n",
    "                                y_score = full_gbm_default_pred).round(4)\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', gbm_train_score)\n",
    "print('Testing ACCURACY :', gbm_test_score)\n",
    "print('AUC Score        :', gbm_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the results\n",
    "model_performance.append(['Gradient Boosting Default',\n",
    "                          gbm_train_score,\n",
    "                          gbm_test_score,\n",
    "                          gbm_auc_score])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Observations: </strong>\n",
    "- improved model performance\n",
    "- run hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "### GridSearchCV\n",
    "##########################################\n",
    "## declaring a hyperparameter space\n",
    "#learning_rate_space     = pd.np.arange(0.05,0.5,0.05)\n",
    "#n_estimators_space      = pd.np.arange(100,500,50)\n",
    "#min_samples_split_space = pd.np.arange(0.1,1,0.1)\n",
    "#min_samples_leaf_space  = pd.np.arange(1,5,1)\n",
    "#max_depth_space         = pd.np.arange(1,10,1)\n",
    "#warm_start_space              = [True, False]\n",
    "#\n",
    "## creating a hyperparameter grid\n",
    "#param_grid = {'learning_rate'     : learning_rate_space,          \n",
    "#              'n_estimators'      : n_estimators_space,\n",
    "#              'min_samples_split' : min_samples_split_space,\n",
    "#              'min_samples_leaf'  : min_samples_leaf_space,\n",
    "#              'max_depth'         : max_depth_space,\n",
    "#              'warm_start'        : warm_start_space\n",
    "#             } \n",
    "#\n",
    "#\n",
    "## INSTANTIATING the model object without hyperparameters\n",
    "#GB_tuned = GradientBoostingClassifier(random_state = seed)\n",
    "#\n",
    "## GridSearchCV object\n",
    "#knn_tuned_cv = GridSearchCV(estimator  = GB_tuned, \n",
    "#                            param_grid = param_grid, \n",
    "#                            cv         = 3, \n",
    "#                            scoring    = make_scorer(roc_auc_score,\n",
    "#                                                    needs_threshold = False))\n",
    "#\n",
    "#\n",
    "## FITTING to the FULL DATASET (due to cross-validation)\n",
    "#knn_tuned_cv.fit(chef_imp_stand, chef_target.values.reshape(-1,))\n",
    "#\n",
    "## printing the optimal parameters and best score\n",
    "#print(\"Tuned Parameters  :\", knn_tuned_cv.best_params_)\n",
    "#print(\"Tuned CV AUC      :\", knn_tuned_cv.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuned parameters: {'learning_rate': 0.01, 'max_depth': 9, 'min_samples_leaf': 1, 'min_samples_split': 0.6, 'n_estimators': 900}\n",
    "Tuned CV AUC      : 0.6406"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient boosting selects the best features that can explain our model. Here, we will use the cut-off at 0.01 to determine these features based on the box plots analysis we did earlier. Variable before this value did not show a significant separation between failure and success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## storing important features from gradient boosting as df \n",
    "#gb_features = pd.DataFrame(full_gbm_default_fit.feature_importances_)\n",
    "#gb_features['Variable Names'] = chef_full_stand.columns\n",
    "#\n",
    "## Looking at insignificant coefficients\n",
    "#gb_features = gb_features.iloc[:,:][gb_features[0] >= 0.01].sort_values(by = 0) # cut-off based on boxplots analysis\n",
    "#gb_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.8307\n",
      "Testing ACCURACY : 0.7864\n",
      "AUC Score        : 0.7666\n"
     ]
    }
   ],
   "source": [
    "###### Standardized Preparation \n",
    "# Standardizing our Data Set (only numeric variables) with user-defined function\n",
    "chef_stand = standard(chef)\n",
    "\n",
    "# Defining explanatory variables (add according to new feature selections)\n",
    "chef_gb_stand   = chef_stand.loc[: , variables_dict['Best Model']]\n",
    "\n",
    "# train-test split with stratification\n",
    "X_train_stand, X_test_stand, y_train_stand, y_test_stand = train_test_split(\n",
    "            chef_gb_stand,   # change\n",
    "            chef_target,\n",
    "            test_size = 0.25,\n",
    "            random_state = seed,\n",
    "            stratify = chef_target) \n",
    "\n",
    "# merging training data for statsmodels\n",
    "chef_train_stand = pd.concat([X_train_stand, y_train_stand], axis = 1)\n",
    "\n",
    "\n",
    "# INSTANTIATING the model object with hyperparameters\n",
    "fit_gbm_tuned = GradientBoostingClassifier(learning_rate = 0.01,\n",
    "                                           n_estimators = 900,\n",
    "                                           min_samples_split = 0.6,\n",
    "                                           min_samples_leaf = 1,\n",
    "                                           max_depth = 9,\n",
    "                                           warm_start = False,\n",
    "                                           random_state = seed)\n",
    "\n",
    "# FIT step is needed as we are not using .best_estimator\n",
    "fit_gbm_tuned_fit = fit_gbm_tuned.fit(X_train_stand, y_train_stand.values.reshape(-1,))\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "fit_gbm_tuned_pred = fit_gbm_tuned_fit.predict(X_test_stand)\n",
    "\n",
    "fit_gbm_train_score = fit_gbm_tuned_fit.score(X_train_stand, y_train_stand).round(4)\n",
    "fit_gbm_test_score  = fit_gbm_tuned_fit.score(X_test_stand, y_test_stand).round(4)\n",
    "fit_gbm_auc_score   = roc_auc_score(y_true  = y_test_stand,\n",
    "                                   y_score = fit_gbm_tuned_pred).round(4)\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', fit_gbm_train_score)\n",
    "print('Testing ACCURACY :', fit_gbm_test_score)\n",
    "print('AUC Score        :', fit_gbm_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the results\n",
    "model_performance.append(['Gradient Boosting Fit',\n",
    "                          fit_gbm_train_score,\n",
    "                          fit_gbm_test_score,\n",
    "                          fit_gbm_auc_score])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Testing Accuracy</th>\n",
       "      <th>AUC Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.7629</td>\n",
       "      <td>0.7392</td>\n",
       "      <td>0.6997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression - Standardized</td>\n",
       "      <td>0.7820</td>\n",
       "      <td>0.7988</td>\n",
       "      <td>0.7655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression - Tuned</td>\n",
       "      <td>0.7800</td>\n",
       "      <td>0.7988</td>\n",
       "      <td>0.7655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Full KNN</td>\n",
       "      <td>0.8225</td>\n",
       "      <td>0.6509</td>\n",
       "      <td>0.6754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNN Tuned</td>\n",
       "      <td>0.8225</td>\n",
       "      <td>0.6509</td>\n",
       "      <td>0.6754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tuned Tree</td>\n",
       "      <td>0.9075</td>\n",
       "      <td>0.8973</td>\n",
       "      <td>0.9058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Fit Tree</td>\n",
       "      <td>0.8095</td>\n",
       "      <td>0.7864</td>\n",
       "      <td>0.7836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Random Forest Default Full</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.7454</td>\n",
       "      <td>0.7025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Random Forest Tuned Full</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.7598</td>\n",
       "      <td>0.7250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Random Forest Tuned Fit</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.7659</td>\n",
       "      <td>0.7380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Gradient Boosting Default</td>\n",
       "      <td>0.9034</td>\n",
       "      <td>0.7659</td>\n",
       "      <td>0.7431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Gradient Boosting Fit</td>\n",
       "      <td>0.8307</td>\n",
       "      <td>0.7864</td>\n",
       "      <td>0.7666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Model  Training Accuracy  Testing Accuracy  AUC Value\n",
       "0                  Logistic Regression             0.7629            0.7392     0.6997\n",
       "1   Logistic Regression - Standardized             0.7820            0.7988     0.7655\n",
       "2          Logistic Regression - Tuned             0.7800            0.7988     0.7655\n",
       "3                             Full KNN             0.8225            0.6509     0.6754\n",
       "4                            KNN Tuned             0.8225            0.6509     0.6754\n",
       "5                           Tuned Tree             0.9075            0.8973     0.9058\n",
       "6                             Fit Tree             0.8095            0.7864     0.7836\n",
       "7           Random Forest Default Full             1.0000            0.7454     0.7025\n",
       "8             Random Forest Tuned Full             1.0000            0.7598     0.7250\n",
       "9              Random Forest Tuned Fit             1.0000            0.7659     0.7380\n",
       "10           Gradient Boosting Default             0.9034            0.7659     0.7431\n",
       "11               Gradient Boosting Fit             0.8307            0.7864     0.7666"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# declaring a DataFrame object\n",
    "model_performance_df = pd.DataFrame(model_performance[1:], columns = model_performance[0])\n",
    "model_performance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the table above, the best model to predict if a customer will or not subscribe to Halfway There is the decision tree. In the following steps, we will interpret the results of the model and evaluate is applicability to the business case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAEWCAYAAABR3S+vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deZwcRf3/8dd7k0CAcJ/hDHcEhQABQTkihwIihwcBkUshKCCiiEbkBwHki8qliCLhEAiHXHKDgIFwyH0FgpxyBgI5iEAghGT5/P6o2jCZzM7Obna3d4b3M49+ZKa6u6q6Z3Y+XdXV3YoIzMzMrPs1FV0BMzOzzyoHYTMzs4I4CJuZmRXEQdjMzKwgDsJmZmYFcRA2MzMriIOwNRRJC0i6QdK7kq6ch3z2knRbZ9atCJJukbRvF+S7m6TXJU2TtEFn599G2U9LGtLGMivnuvXqpmr1CJIuljSii/Lulffpyvn9gpJuyn9rl0naV9ItXVF2I3MQtkJI+q6kR/If9YQcLDbvhKy/DSwLLBkR3+loJhFxSUR8tRPqMwdJQySFpH+Upa+f08fUmM8ISRe3tVxE7BARF3awutWcAhwaEf0i4vEK9QtJH+TPd4qk0ZKGdkbBEbFuRIxpY5nXct2aO6NMmCOwt0yl2zhN0hadVVaVOkjS4flA5ANJ4yVdLmndri47IprzPn0tJw0FliD9re0ZERdGxA5dXY9G4yBs3U7Sz4A/AP9HCpgrA38BdumE7FcBno+IWZ2QV1eZBHxJ0pIlafsCz3dWAfnHuiv/vlcBnm5jmfUjoh+wNnABcKakY7uwTl2qJLD3y9sFeRvzdE/5Ol3QEv8zcEieFgfWAm4Evt7J5dRiFeC5zvhb+6z1WMwhIjx56rYJWBSYBnynyjLzk4L0m3n6AzB/njcEGA8cAUwEJgD753nHAR8DM3MZPwBGABeX5D0ACKB3fr8f8BLwPvAysFdJ+r0l630JeBh4N///pZJ5Y4ATgH/nfG4Dlmpl21rq/1fgkJzWK6cdA4wpWfaPwOvAe8CjwBY5ffuy7RxbUo8Tcz2mA2vktAPy/LOAq0ry/x0wGlCFejYBRwOv5v18Uf7s5s9lBvAB8N9WtjOANcrSvg18RGo5tXwXzsuf4RvAb4BeJcsfCDyT9+l/gA1z+ivAtvn1JsAjeR+9DZzWyue8PHA98A7wInBgSTkjgCvyNr5POrgYXMN3udI2XkwKlP/M+2cI0Bc4LX+Wb5MOOPuWrLMzMBb4H3Av8PlWyhsINLfsh1aWuRgYkV8vCdxMOuibCtwArFCy7A/yvnyf9DewR05fC7ib9F2fDFya03vnbR5A+p6Vfgf3BQ5gzu/vOsC/8j5/FvhWtf1U9G9TUVPhFfD02ZpIAWRWy49jK8scDzwALAMsDdwHnJDnDcnrHw/0AXYEPgQWz/NHMGfQLX8/+8cZWIj04712ntcfWDe/3o8chEldblOBvfN6e+b3LcFkDPDf/OO1QH7/21a2bQgp4H4JeDCn7QjcWuFH7Hv5h7Q36aDjLfKPd/l2ldTjNWDdvE4f5gzCC5Ja2/sBW+Qf2BVbqef3ScFqNaAf8A9gVMn8uQJQ2fqVAlSf/NntkN9fC5ydP4dlgIeAg/K875AC88aASAcUq+R5r/BpEL4f2Du/7gdsWv455/d3kYMfMIgUmLYp2Zcf5c+hF3AS8EAN3+XWgvBUYDPSgcz8wJnANaSW6yKkwNjyfd6YFJg3zmV/P3+X5qtQ3qG0ctBTVv6I/HppYDfSd3KR/BlelectQgqya5Z899fJr68Efpnr3xf4ck6fHYTz+98AF5SUPfv7CyycP7998nobAVP49G9trv1U9G9TUZO7o627LQlMjupdWHsBx0fExIiYRGrh7l0yf2aePzMibiYdia/dwfp8Anxe0gIRMSEiKnWxfh14ISJGRcSsiLiMdGT/jZJl/hYRz0fEdFKralC1QiPiPmAJSWuTfqguqrDMxRExJZd5KukHva3tvCAins7rzCzL70NSYD+N9CP444gY30o+e5FalS9FxDTgV8Aeknq3UX6rcn0mk7Z7WWAH4PCI+CAiJgKnA3vkxQ8Afh8RD0fyYkS8WiHbmcAakpaKiGkR8UD5ApJWAjYHfhkRH0XEE8C5zPmdujcibo50DnkUsH5HtxO4JiLuj4hPcv0OyNs5NSLeIwX5lu0cBvwlb2dzRJyf0zeukO+SpF6DmkTEpIi4JiKm53L/D9iqdBHSd79v/u7/J6fPJB3E9M/769+1llliZ9JpoYvyd/FR0kHXt0uWmb2fImJGB8poCA7C1t2mAEu18WO+PKkbtMWrOW12HmVB/ENSK6hdIuID0uCSHwIT8kjPgTXUp6VOK5S8f6sD9RlFat18hdRSmoOkIyQ9k0ef/o/UfbtUG3m+Xm1mRDxE6noU6WChNZU+g96kc/gdIqkPqXX2Dul8Yh/Sfv9f3r6zSS1igJVILcK2/IDUA/GspIcl7VRhmeWBdyLi/ZK0tj6/vvNwwFH6GSxHOngaW7KdN/Lpdq4C/LJlXp7fv6xuLabkeTWRtJCkcyW9Juk94A7y9ycH5T1J55bfknSjpLXyqkeQPptHJD3VwdH1qwBfLtuuoWX1r/pd/axwELbudj+p62/XKsu8SfojbrFyTuuID0jdsC2WK50ZEbdGxHakH4dngXNqqE9Lnd7oYJ1ajAIOBm7OrdTZ8kjbXwK7k7raFyN1H6ql6q3kWfWxaJIOIQWFN4FfVFm00mcwi9R12lG75DweIv0AzyCdO18sT4tERMso39eB1dvKMCJeiIg9SUHtd8BVkhaqsC1LSFq4bHvm9fNrtVolr98mnTtdu2Q7F42IRfP814HjSuYtFhELRkSlA6TRwIB2XBL2C2BVYJOIWATYeo5KRtwSEduSvvsvkg6CyK3iAyKiPylIj5S0ao1ltngdGF22Xf0i4tDSKrQzz4bkIGzdKiLeJQ1A+rOkXfO1hn0k7SDp93mxy4CjJS0taam8fJuX47TiCWDLfHnJoqRuVQAkLStp5/yjPYPUrV3pkpabgbXyZVW986U265BaNB0WES+Tugd/XWH2wqSANQnoLekY0nm8Fm+TfpBr/hvOLZ3fkLqk9wZ+Iam1bvPLgJ9KWlVSP1JX5uVtnEZordwlJO1FGojzu9zFPoE0gO1USYtIapK0uqSW7tJzgZ9L2iiP9F5DUvmBEJK+J2np3PX7v5w8x2cYEa+TxhWcJKmvpPVILehL2rst7ZW7t88F/pC/z5K0oqSWy99GAodI2jjP6yfpGxUOJIiIZ/Lyl0vaStJ8StfFf1fSkRWKX5jUqp+aR+If0zJDUv9czoKkg4QPyPtN0u6SWlri/yMFy/Ze6nU9sG6uW588bZJPv1gJB2HrdhFxGvAz0ujbSaSj5kNJ54wgBYpHgCeBp4DHclpHyroduDzn9ShzBs4mUtfbm6Qu0q1ILdPyPKYAO+Vlp5BaGDtFxOSO1Kks73sjolIr/1bgFtJAqldJvQel3XctNyKZIumxtsrJXasXk4Lg2Ih4ATgKGCVp/gqrnE9qqd9NGjX+EfDj2rZqtrGSppFaWQcAP42IY0rm7wPMRxr5PBW4itxdGRFXkkbgXkoavXstaYBcue2Bp3M5fySN8P2ownJ7ks5zvknq+j82fze6wxGkz/AhUm/GbcCaABHxIPAj0sj1qaTP+3tV8jokL9uy/Auk8683VVj2NNIpjCmkg5DSG2n0Ao4knWOeQhoo2NJK/SLwsKQPSIO5DolPrw2uST7Y/lrelgmk7v6TSL0wVkIR7hEwMzMrglvCZmZmBXEQNjMzK4iDsJmZWUEchM3MzArS4bvfmNXqyBuf8+g/m8sJ2/tqFZtb396zr4XvsAU2OLTm35zpj585z+XNC7eEzazbOQCbJW4Jm5lZY+nSp3h2LgdhMzNrLE3183hiB2EzM2ssKvQ0b7s4CJuZWWNxd7SZmVlB3BI2MzMriFvCZmZmBXFL2MzMrCAeHW1mZlYQd0ebmZkVxN3RZmZmBXFL2MzMrCAOwmZmZgXp5YFZZmZmxfA5YTMzs4K4O9rMzKwgbgmbmZkVxC1hMzOzgrglbGZmVhDfttLMzKwg7o42MzMriLujzczMCuKWsJmZWUEchM3MzArigVlmZmYF8TlhMzOzgrg72szMrCBuCZuZmRVDdRSE66fNbmZmVgNJNU9t5LOSpDslPSPpaUk/yekjJL0h6Yk87Viyzq8kvSjpOUlfa6uubgmbmVlDUVOntYRnAUdExGOSFgYelXR7nnd6RJwyR7nSOsAewLrA8sC/JK0VEc2tFeCWsJmZNZTOaglHxISIeCy/fh94Blihyiq7AH+PiBkR8TLwIrBJtTIchM3MrKG0JwhLGibpkZJpWCt5DgA2AB7MSYdKelLS+ZIWz2krAK+XrDae6kHbQdjMzBpLe4JwRIyMiMEl08gK+fUDrgYOj4j3gLOA1YFBwATg1JZFK1QnqtXVQdjMzBqL2jG1lZXUhxSAL4mIfwBExNsR0RwRnwDn8GmX83hgpZLVVwTerJa/g7CZmTWUThwdLeA84JmIOK0kvX/JYrsB4/Lr64E9JM0vaVVgTeChamV4dLSZmTWUpqZOa19+GdgbeErSEzntKGBPSYNIXc2vAAcBRMTTkq4A/kMaWX1ItZHR4CBsZmYNprNu1hER91K50/rmKuucCJxYaxkOwmZm1ljq54ZZDsJmZtZY6um2lQ7CZmbWUByEzczMCtKJt63scg7CZmbWUNwSNjMzK4iDsJmZWUEchM3MzAriIGxmZlaU+onBDsJmZtZYOvG2lV3OQdjMzBqKu6PNzMyKUj8x2I8yNOtMj//9j/zz2L258+RD55r34p3XcP0ROzNj2nsATH7xKW7+9R6MOfUnjDn1Jzx329+7u7pWgGOO/hVDttiMb+6y0+y02269hd12/jqDPj+Qp8c9VWDtGkNnPcqwO3RZEJbULOkJSU9LGivpZ5Ka8rzBks5oZb1XJC3VSvrVJe+/LemCNuowRNKXWpm3n6RJJXW8StKC7drIyvk2STpD0jhJT0l6OD9Xsto6F0j69ryWXZbnzpKGt7HMIEk7tmcdq27ljbdh0wNHzJU+feokJj3/BAssvvQc6Uuuug5DjvgjQ474I2t/dY9uqqUVaZddv8lZZ587R9oaa6zF6X/8ExsN3rigWjUWB+FkekQMioh1ge2AHYFjASLikYg4rAN5Dpa0bjuWHwJUDMLZ5SV1/BgY2oE6lRsKLA+sFxFfID3w+X+dkG/NJPWOiOsj4rdtLDqI9LkAUOM6VsWSq3+e+RbsN1f6uOvPY51v7Edd9ZNZl9ho8MYssuiic6SttvrqDFh1tYJq1HgchMtExERgGHCokiGSbgSQtKSk2yQ9Lulsqv9KnUJ6oPIcJC0h6VpJT0p6QNJ6kgYAPwR+mlu7W7SWqaTewELA1Px+jpappGn5/1GSdilJv0TSzmXZ9QcmRMQnedvHR8TU0nzy6/KW/LaS7pH0vKSd8jLrSnoo1/9JSWvm9H3y+7GSRpXU+TRJdwK/yy39M0vm/bU0f0nzAccDQ3P+Q8vWWUXS6FzOaEkrl+R1hqT7JL3U2S34RvTWuAfpu+iSLLr83B0i77z6HGNOOYwHzhnBe2+9VkDtzBqPmlTzVLRuOyccES/l8pYpm3UscG9EbABcD6xcJZsrgA0lrVGWfhzweESsRwrSF0XEK8BfgdNza/eeCvkNlfQE8AawBHBDG5txLrA/gKRFSa3s8oc7XwF8Iwe2UyVt0EaeLQYAWwFfB/4qqS/pIOKPETEIGAyMzz0Bvwa2joj1gZ+U5LEWsG1EHNFW/qTP4hg+7Q24vGz5M0n7cT3gEqD09EF/YHNgJ6Biy1nSMEmPSHpk7D/Ls/7smPXxDJ4ffSUDv/bdueYtuuLqbHf0uQz5+RmsuvlOPPy3mp8DbmZVuCXcukpbvCVwMUBE3ERujbaiGTgZ+FVZ+ubAqJzHHcCSOUi25fIc4JYDngKOrLZwRNwFrCFpGWBP4OqImFW2zHhg7VzHT4DRkrapoS5XRMQnEfEC8BIwELgfOErSL4FVImI6sDVwVURMzuW9U5LHlRHR3I78q9kMuDS/HkXaxy2uzXn9B1i20soRMTIiBkfE4PW374xe/vr04ZQJfPjO24w59Sfc/psD+Ojdydx9+uF89N5U+vRdkN7zLwDAsp8bzCfNzbMHbZlZx9VTEO62S5QkrUYKohOBz5XNjnZkNYoU4J4uzb7CcjXnGREh6Qbgx6SW3SzyAYrSpzRfWfl7AXsA328lvxnALcAtkt4GdgVGl9Wpbxv1jYi4VNKDpNbrrZIOIG1ra9v2QbXNbON9W0qXn1HyuvhvcQ+2SP8BbH/cqNnvb//NAWx5+GnM328RPnpvKvMvvBiSmPra8xCfMN9CCxdYW7PG0ANia826pSUsaWlSF+iZEVH+4383KaghaQdg8Wp5RcRM4HTg8FbyGAJMjoj3gPeBWn/VNgf+m1+/AmyUX+8C9ClZ7oKWsiOi9ECAXP6GkpbPr5uA9YBX8+y3JX0up+9Wtup3lEZWrw6sBjyXD1xeiogzSF3165GC+e6SlsxlLFHj9s2VP9X3z32kAw1I+/beGsv5THt01Mncc8YvmDbxDW47fn9effC2Vped8OS/GXPyoYw55TDGXTOSjb53ZI84Mreu9cuf/4x9vrsHr77yMtttvSX/uPpKRv/rdrbbekvGPvE4hx58ED888AdFV7OuuSWcLJDPt/YhtSxHAadVWO444DJJjwF3AbWMTjkPOLrk/Qjgb5KeBD4E9s3pNwBX5cFUP65wXniopM1JByPjgf1y+jnAdZIeIgW92S3MiHhb0jPAta3UbRngHEnz5/cPkc6vAgwHbgReB8YBpcNonyNt/7LADyPiI0lDge9Jmgm8BRwfEe9IOhG4S1Iz8HhJvauplP+dwPD8OZ1UtvxhwPmSjgQmkc+FW3Ub7V31jAbbHf3ppSmrbr4Tq26+U5WlrRH97pRKP4OwzbbbdXNNGldTDxhwVSvN3TC1apSuJX4K2DAi3i26PrXIo7BvjIiriij/yBuf85fM5nDC9msXXQXrofr2nvdTXAOH31rzb86zv/1aoRHbd8xqB0nbAs8Cf6qXAGxm9lnT1KSap6L53tHtEBH/ovolVD1SROxXdB3MzLpLDzjVWzMHYTMzayg9YcBVrRyEzcysodRRDHYQNjOzxtLUVD/DnRyEzcysobglbGZmVpB6OidcP212MzOzGki1T9Xz0UqS7pT0jNJz53+S05eQdLukF/L/i+d05afMvZifQLdhW3V1EDYzs4bSibetnAUcERGfAzYFDpG0Dunuh6MjYk3SXRWH5+V3ANbM0zDgrLYKcBA2M7OG0lkt4YiYEBGP5dfvA88AK5CeKXBhXuxC0kN6yOkXRfIAsJik/tXK8DlhMzNrKF1xJyxJA4ANgAeBZSNiAqRAnR9vCylAv16y2vicNqHVunZ6Tc3MzArUnu5oScMkPVIyDauQXz/gauDw/IS+VouukFb1PtZuCZuZWUNpz+DoiBgJjGw9L/UhBeBLIuIfOfltSf1zK7g/MDGnjwdWKll9ReDNauW7JWxmZg2lswZmKS1wHvBMRJQ+g/J6Pn1k7r7AdSXp++RR0psC77Z0W7fGLWEzM2sonXiZ8JeBvYGn8nPXAY4CfgtcIekHwGvAd/K8m4EdgRdJz7Zv8znsDsJmZtZQOmtgVkTcS+XzvADbVFg+gEPaU4aDsJmZNZR6umOWg7CZmTUUB2EzM7OC1FEMdhA2M7PG4pawmZlZQeooBjsIm5lZY+mK21Z2FQdhMzNrKE111BR2EDYzs4ZSRzHYQdjMzBqLB2aZmZkVpI5OCTsIm5lZY/HALDMzs4Ko1ds99zwOwmZm1lDqqCHsIGxmZo3FA7PMzMwKUkcx2EHYzMwai2/WYWZmVhCPjjYzMytIHTWEHYTNzKyxuDvazMysIPUTgqsEYUk3ANHa/IjYuUtqZGZmNg8a5RKlU7qtFmZmZp2kjsZltR6EI+Ku7qyImZlZZ2io0dGS1gROAtYB+rakR8RqXVgvMzOzDqmn7uimGpb5G3AWMAv4CnARMKorK2VmZtZRTap9KlotQXiBiBgNKCJejYgRwNZdWy0zM7OOkVTzVLRaLlH6SFIT8IKkQ4E3gGW6tlpmZmYdU3xorV0tQfhwYEHgMOAEUit4366slJmZWUf16gn9zDVqMwhHxMP55TRg/66tjpmZ2bzpCd3MtapldPSdVLhpR0T4vLCZmfU4dRSDa+qO/nnJ677At0gjpc3MzHqczrx3tKTzgZ2AiRHx+Zw2AjgQmJQXOyoibs7zfgX8AGgGDouIW6vlX0t39KNlSf+W5Bt5mJlZj9TJLeELgDNJl+eWOj0i5rizpKR1gD2AdYHlgX9JWisimlvLvJbu6CVK3jYBGwHL1VR1M+AnX1616CpYDzP5/Y9Zc+ufFV0N64GmP37mPOfRmeeEI+JuSQNqXHwX4O8RMQN4WdKLwCbA/a2tUEt39KOkc8IidUO/TGpqm5l1iAOwdaVe7QjCkoYBw0qSRkbEyBpWPVTSPsAjwBERMRVYAXigZJnxOa1VtQThz0XER2WVnr+G9czMzLpde65QygG3lqBb6izSJbuR/z8V+D6VL1Fu9WmEUNsds+6rkNZq09rMzKxIXX3byoh4OyKaI+IT4BxSlzOklu9KJYuuCLxZLa9qzxNejtSMXkDSBnwa4Rch3bzDzMysx+nq64Ql9Y+ICfntbsC4/Pp64FJJp5EGZq0JPFQtr2rd0V8D9iNF8lP5NAi/BxzVoZqbmZl1sc68YZaky4AhwFKSxgPHAkMkDSJ1Nb8CHAQQEU9LugL4D2kM1SHVRkZD9ecJXwhcKOlbEXF1J2yLmZlZl+vMhnBE7Fkh+bwqy58InFhr/rWcE95I0mItbyQtLuk3tRZgZmbWnXpLNU9FqyUI7xAR/2t5k4dh79h1VTIzM+s4qfapaLVcotRL0vz54mMkLQD4EiUzM+uROvO2lV2tliB8MTBa0t/y+/2BC7uuSmZmZh1XRzG4pntH/17Sk8C2pBHS/wRW6eqKmZmZdUQdPU64ppYwwFvAJ8DupNtWerS0mZn1SL3qKApXu1nHWqSnQewJTAEuBxQRX+mmupmZmbVbHcXgqi3hZ4F7gG9ExIsAkn7aLbUyMzPrIFW8hXPPVO0SpW+RuqHvlHSOpG2ofHNqMzOzHqOr7x3dqXVtbUZEXBMRQ4GBwBjgp8Cyks6S9NVuqp+ZmVm7NEQQbhERH0TEJRGxE+k+0k8Aw7u8ZmZmZh0gqeapaLWOjgYgIt4Bzs6TmZlZj9OrlntB9hDtCsJmZmY9XaPdMcvMzKxu9IRzvbVyEDYzs4ZSRw1hB2EzM2ssTXV0Na2DsJmZNRS3hM3MzArSu45OCjsIm5lZQ3FL2MzMrCC+RMnMzKwgdRSDHYTNzKyx1NENsxyEzcyssbg72szMrCAOwmZmZgWpnxDsIGxmZg2mjhrCDsJmZtZYesJzgmvlIGxmZg3Fo6PNzMwK4oFZZmZmBamn7uh6arWbmZm1qakdU1sknS9poqRxJWlLSLpd0gv5/8VzuiSdIelFSU9K2rCWupqZmTUMSTVPNbgA2L4sbTgwOiLWBEbn9wA7AGvmaRhwVluZOwibmVlDUTumtkTE3cA7Zcm7ABfm1xcCu5akXxTJA8BikvpXy99B2MzMGkovqeZJ0jBJj5RMw2ooYtmImACQ/18mp68AvF6y3Pic1ioPzDIzs4bSnnFZETESGNlZRVcqotoKbgmbmVlDUTv+ddDbLd3M+f+JOX08sFLJcisCb1bLyEHYzMwailT71EHXA/vm1/sC15Wk75NHSW8KvNvSbd0ad0ebmVlDaerERzhIugwYAiwlaTxwLPBb4ApJPwBeA76TF78Z2BF4EfgQ2L+t/B2EzcysoXTmvToiYs9WZm1TYdkADmlP/g7CZmbWUHzbSjMzs4I01U8MdhA2M7PGMg+jnrudg7CZmTWUOuqNdhA26wofz5jB4T/aj5kff0xzczNbbr0d+x14CI8/8iB/PeNUZs2ayZoD1+HIo46jV2//GTayFZddjHNP2Idll1yETyI4/+p/8+fLxvCFtVbgT7/eg4UWmJ9X35zC/r++kPc/+IitvziQEw7bmfn69ObjmbM46g/XctfDzxe9GXWlnlrCSoO56o+kZuAp0h1KmoFDI+K+Ti5jBDAtIk4pS18M+G5E/KWV9X4NfDfX6xPgoIh4sL3lzAtJg4F9IuKwKssMAL4UEZfWuk5HjJ/6cX1+yeZBRPDR9OkssOCCzJo1k58M25eDD/8FJxz9c04+81xWWnkAfxt5Jssutzw77vzNoqvb7dbc+mdFV6HbLLfUIiy31CI88ex4+i04P/dd+kt2/9lIzj1+b4affg33Pvoi++yyKQNWWJLj/3IT66+9IhPfeZ8Jk95lndX7c8NfDmH1rx1d9GZ0m+mPnznPEfTu59+p+Tdny7WWKDRi1/PNOqZHxKCIWB/4FXBSN5a9GHBwpRmSNgN2AjaMiPWAbZnzXqJdTlLviHikhmA6gHSwAECN61gNJLHAggsCMGvWLGbNmkVTUxN95puPlVYeAMBGm2zGPXfeXmAtrTu8Nfk9nnh2PADTPpzBsy+/xfJLL8aaqyzDvY++CMAdDzzLrtsMAmDsc+OZMOldAP7z3wnMP18f5uvj3pL2aJJqnopWz0G41CLAVJj9PMeTJY2T9JSkoTl9iKQbW1aQdKak/fLrHSU9K+ne/CzIG0vyXkfSGEkvSWoJUL8FVpf0hKSTy+rSH5gcETMAImJyRLyZy3lF0lL59WBJY0rWW1/SHfn5lAfmZfpLujuXM07SFjl9e0mPSRoraXROGyFppKTbgItKtzfPG1Wef96OLXL+Py1bZwlJ1+ZnYj4gab2SvM6vsE+sTHNzM8P2/jbf2mErNtpkUwau+wVmzZrFc888DcDdd9zOpIlvFVxL604r91+CQWuvyMPjXuE//53ATkO+AMA3t9uQFZddfK7ld9t2EGOfe52PZ87q7qrWtc58ilJXq+fDqwUkPQH0JQW+rXP6N4FBwBgUbZkAABGaSURBVPrAUsDDku5uLRNJfYGzgS0j4uV8d5RSA4GvAAsDz0k6i/TsyM9HxKAKWd4GHCPpeeBfwOURcVcN27MesCmwEPC4pJuAPYFbI+JESb2ABSUtDZxTUt8lSvLYCNg8IqZLGlJD/sOBn0fETnlflK5zHPB4ROwqaWvgItJ+rbhPImJmaWH5SSTDAH572p/Za78DatgFjaVXr16MHHUV095/j2N+eTivvPQiR5/we/7yh98zc+bHDN5kM3r1quc/QWuPhRaYj8tOOYAjT7ma9z/4iINGXMKpv/g2vzpwB2666yk+ntk8x/KfW205fnPYLux08J8LqnH96gkt3FrV8y/A9JYgmLuAL5L0eWBz4LKIaCbdZPsuYGPgvVbyGQi8FBEv5/eXkYNHdlNu1c6QNBFYtlqlImKapI2ALUiB6nJJwyPigja257qImA5Ml3QnsAnwMHC+pD7AtRHxRA6Ud7fUNyJKn3N5fc6j1vz/V6U+mwPfymXcIWlJSYvmeZX2yfiy/TD7ySSfxXPCpfotvAiDNtyYhx/4N7vvtR9/PDs9hvSRB+9j/OuvFlw76w69ezdx2SkHcvktj3DdHWMBeP6Vt/lGDrBrrLwMO2yx7uzlV1hmMS4/bRgH/L9RvDx+ciF1rmf1E4IbpDs6Iu4ntXqXpvX9P4s5t7dv/r+tz2tGyetmajhwiYjmiBgTEccCh5KDWVkd+pavNnc2cTewJfAGMErSPrm+rQW1D6pVq4335ao9kqvd++Sz5n9T32Ha++m4b8ZHH/Howw+w0iqrMvWdKQB8/PHH/H3U+Xxjt92LrKZ1k78euxfPvfwWZ1x8x+y0pRfvB6TxA8MP/BrnXHUvAIv2W4B//OmHHPOn67l/7EuF1Lfu1VF/dEP8eEoaCPQCpgB3AwdJuhBYghTEjgT6kM7vzk8KgNsA9wLPAqtJGhARrwBDayjyfVJXbKW6rA18EhEv5KRBQEtz5xVSl/EtfBqYW+wi6SRSd/EQYLikVYA3IuIcSQsBGwInAn+WtGpLd3RZa7g1c+VP6savuB2k/bgXcEJufU+OiPdUR908RZoyeRK/P+FompubiQi22uarbLb5Vpz9p1N54N67+CSCnb+5OxsM/mLRVbUu9qVBq7HXTl/kqeff4IG/Dwfg2DOvZ42VluGgoVsCcN0dT3DRdQ8A8MM9tmT1lZZm+IHbM/zA7QH4xo/OZNLUacVsQB1yd3T3aDknDOl4Zt+IaJZ0DbAZMJbUcvtFRLwFIOkK4EngBeBxgHz+9GDgn5ImAw+1VXBETJH0b0njgFsi4siS2f2AP+XLmGaRnqbR0r19HHCepKOA8kuWHgJuAlYGToiINyXtCxwpaSYwjXT50KR8vvUfkppIz7Hcrob9VSn/ScAsSWOBC1r2STYC+JukJ0lPA9kXq9nqa67N2RddOVf6QT8+goN+fEQBNbKi3PfESyywwaFzpd/Kf/jzZWPmSv/dubfyu3Nv7YaaNa76CcF1fJ1wZ5LUL5/LFfBn4IWIOL3oenUWdcF1yO3xWT8nbHP7LF0nbO3TGdcJP/zyuzX/5my86qK+TrgHODC3qp8GFiWNljYzszqkdvwrWj13R3ea3OptmJZvuYgYUXQdzMy6Sx2dEnYQNjOzxlJHMdhB2MzMGks9XcXhIGxmZg2ljmKwg7CZmTWWOorBDsJmZtZg6igKOwibmVlD6QmXHtXKQdjMzBqKzwmbmZkVxEHYzMysIO6ONjMzK4hbwmZmZgWpoxjsIGxmZg2mjqKwg7CZmTWUpjrqj3YQNjOzhtKZIVjSK8D7QDMwKyIGS1oCuBwYALwC7B4RUzuSv58nbGZmjUXtmGrzlYgYFBGD8/vhwOiIWBMYnd93iIOwmZk1FLXjXwftAlyYX18I7NrRjByEzcysoUi1TzUI4DZJj0oaltOWjYgJAPn/ZTpaV58TNjOzhtKe9m0OrMNKkkZGxMiS91+OiDclLQPcLunZTqlk5iBsZmYNRe0YHZ0D7sgq89/M/0+UdA2wCfC2pP4RMUFSf2BiR+vq7mgzM2sondUdLWkhSQu3vAa+CowDrgf2zYvtC1zX0bq6JWxmZg2lEy9RWha4JresewOXRsQ/JT0MXCHpB8BrwHc6WoCDsJmZNZZOisIR8RKwfoX0KcA2nVGGg7CZmTUUP0XJzMysIHV010oHYTMzayxNDsJmZmZFqZ8o7CBsZmYNxd3RZmZmBamjGOwgbGZmjcUtYTMzs4K057aVRXMQNjOzhlI/IdhB2MzMGkwdNYQdhM3MrLH4jllmZmZFqZ8Y7CBsZmaNpY5isIOwmZk1lqY6OinsIGxmZg2ljmIwTUVXwMzM7LPKLWEzM2so9dQSdhA2M7OG4kuUzMzMCuKWsJmZWUEchM3MzAri7mgzM7OCuCVsZmZWkDqKwQ7CZmbWYOooCvtmHWbW7V6447Siq2ANrEmqeSqaIqLoOph9ZkgaFhEji66H9Sz+Xnx2uSVs1r2GFV0B65H8vfiMchA2MzMriIOwmZlZQRyEzbqXz/tZJf5efEZ5YJaZmVlB3BI2MzMriIOwmZlZQRyErTCSmiU9IelpSWMl/UxSU543WNIZraz3iqSlWkm/uuT9tyVd0EYdhkj6Uivz9pM0qaSOV0lasF0bWTnfJklnSBon6SlJD0tatY11LpD07XktuyzPnSUNb2OZQZJ2bM863ankOzRW0mOtfZbzWMYIST+vkL6YpIOrrPfr/L15Mtfxix0pZ15U+zsqWWaApO+2Zx3rPL5tpRVpekQMApC0DHApsChwbEQ8AjzSgTwHS1o3Ip6ucfkhwDTgvlbmXx4Rh+Y6XgoMBf7WgXqVGgosD6wXEZ9IWhH4YB7zbBdJvSPieuD6NhYdBAwGbgaocZ3uVPod+hpwErBVN5W9GHAw8JfyGZI2A3YCNoyIGfmgcb5uqldLHXrX+Hc0APgu6e+Pefjbsw5wS9h6hIiYSLphwaFKhki6EUDSkpJuk/S4pLOpfmfYU4CjyhMlLSHp2twqeUDSepIGAD8EfppbKlu0lqmk3sBCwNT8fo6WqaRp+f9RknYpSb9E0s5l2fUHJkTEJ3nbx0fE1NJ88uvylvy2ku6R9LyknfIy60p6KNf/SUlr5vR98vuxkkaV1Pk0SXcCv8st/TNL5v21NH9J8wHHA0Nz/kPL1llF0uhczmhJK5fkdYak+yS91Nkt+CoW4dPPR5JOLultGJrTZ3+v8vszJe2XX+8o6VlJ9+b631iS9zqSxuTtOSyn/RZYPe+bk8vq0h+YHBEzACJickS8mcuZ3ZOTW51jStZbX9Idkl6QdGBepr+ku3M541q+p5K2V2r9j5U0OqeNkDRS0m3ARWV/RyPy93OO/PN2bJHz/2nZOnP93ZTkdX6FfWLt5Jaw9RgR8ZJSd/QyZbOOBe6NiOMlfZ3qdxe6AjhY0hpl6ccBj0fErpK2Bi6KiEGS/gpMi4hTWslvqKTNST+qzwM3tLEZ5wI/Ba6TtCjwJWDfCnW8N/+YjgYujojH28gXUotlK2B14M68jT8E/hgRl+Sg2UvSusCvgS9HxGRJS5TksRawbUQ0twSf1vIH1gCOAQaX9AaUrnMmaT9eKOn7wBnArnlef2BzYCCp5XxVDdvXEQtIegLom8vcOqd/k9SKXx9YCnhY0t2tZSKpL3A2sGVEvCzpsrJFBgJfARYGnpN0FjAc+HxLS7zMbcAxkp4H/kXqUbmrhu1ZD9iUdMD3uKSbgD2BWyPiREm9gAUlLQ2cU1Lf0s94I2DziJguaUgN+Q8Hfh4RLQd2pevM9XdD2q8V90lEzKxhG62EW8LW01Rq5W4JXAwQETeRWzutaAZOBn5Vlr45MCrncQewZA6Sbbk8/8guBzwFHFlt4fxDu4ZS9/qewNURMatsmfHA2rmOnwCjJW1TQ12uiIhPIuIF4CXSj+D9wFGSfgmsEhHTSYHoqoiYnMt7pySPKyOiuR35V7MZuQuTtG83L5l3bc7rP8CyNWxbR02PiEERMRDYntT6U67LZRHRHBFvA3cBG1fJZyDwUkS8nN+XB+GbImJG3qcTaWObImIaKRgOAyYBl1c46KnkuoiYnsu5E9gEeBjYX9II4AsR8T4pkN7dUt+yz/j6/D2oNf9qqv3dtGufWGUOwtZjSFqNFEQnVpjdngvaR5EC98ql2c9LnpEuqL8h5wswi/z3k3/0S8/3jQL2AvanlfPH+cfrlog4Evg/Pm1Bltapbxv1jYi4FNgZmA7cmlsrqrJt1c49z5V/lWXbWn9GyetueVRNRNxPavUuXaXM2Z9b1rKP26pj6fY0U0MvYj4AGBMRxwKHAt+qUIdaPuO7Sd+7N4BRkvah+z7jan837d4nNjcHYesRcvfaX4EzY+47yNxNCmpI2gFYvFpeuUvsdODwVvIYQjpf9x7wPqk7rRabA//Nr18htXQAdgH6lCx3QUvZlQaISdpQ0vL5dROpi/DVPPttSZ/L6buVrfodpZHVqwOrkboAVyO14M4gdfuuR+ri3l3SkrmMJajNXPlTff/cB+yRX+8F3FtjOV1C0kCgFzCF9HkPldQrf7e2BB4i7ed1JM2fW3QtPRDPAqspjROANHiuLa3uG0lrK5+fzwbx6Wf8Cp9+d77FnHaR1Dd/dkNI3eirABMj4hzgPGBDUg/IVsqj6tvxGc+Vf7XtoPW/G+skPnKxIrWcz+tDah2MAio9aPY44DJJj5G6FV+rIe/zgKNL3o8A/ibpSeBDPj1PewNwldJgqh9HxD1l+bScE24CxgP75fRzSOd9HyIFvdmtj4h4W9IzwLWt1G0Z4BxJ8+f3D5HOr0I6P3cj8DowDuhXst5zpO1fFvhhRHykNODoe5JmAm8Bx0fEO5JOBO6S1Aw8XlLvairlfycwPH9OJ5UtfxhwvqQjSV2u+9dQRmdr+Q5BarXtm893X0PqLh9Larn9IiLeApB0BfAk8AJp35DPnx4M/FPSZNJnUlVETJH0b0njgJZejRb9gD9JWoz03X6RT8cyHAecJ+ko4MGybB8CbiL14pwQEW9K2hc4Mn/G04B9ImKSpGHAP/IB20Rguxr2V6X8JwGzJI0lHUCWjk8YQeW/G+skvm2lWSdTupb4KdLlKe8WXZ9aKI3CvjEiumoAVY8nqV9ETMunF/4MvBARpxddr86SzylXG4RoBXB3tFknkrQtqWvzT/USgG22A3Or+mnS9epnF1wf+wxwS9jMzKwgbgmbmZkVxEHYzMysIA7CZmZmBXEQNrNW6dOnFI2TdKXm4SlSmvOexFWfxqQ2nlBUZb1OfxKRWVdyEDazalpuC/l54GPSvapnU9Lu35GIuD4ifltlkZYnFJk1NAdhM6vVPaT7Yg+Q9IykvwCPAStJ+qqk+5We6nOlpH4w+0k/z0q6l/RQBXJ66dOYlpV0jdLTgMYqPRN4ricUSTpS6dnLT0o6riSvX0t6TtK/SPfkNqsbDsJm1ialRznuQLoJCaRgd1FEbEC6W9jRpKczbUh6Fu3PlJ5MdA7wDWAL0kMwKjkDuCsi1ifdkvFp0p3D/ptb4UdK+iqwJumBA4OAjSRtKWkj0q0zNyAF+WoPaTDrcXzbSjOrpvS2kPeQbge6PPBqRDyQ0zcF1gH+nW42xXykexsPBF7OT2VC0sVUfgzl1sA+kB56ALwrqfz+4F/NU8stFfuRgvLCwDUR8WEu4/p52lqzbuYgbGbVTC9/Xm4OtKVP6hFwe0TsWbbcINr/JKbWCDgpIua4i5WkwzuxDLNu5+5oM5tXDwBflrQGpHtnS1qLdPvOVfNTmSA9X7mS0cCP8rq9JC3C3E/2uRX4fsm55hWUntl8N7CbpAUkLUzq+jarGw7CZjZPImIS6SlNl+Wn7TwADIyIj0jdzzflgVmvtpLFT4CvSHoKeBRYNyKmkLq3x0k6OSJuAy4F7s/LXQUsHBGPAZcDTwBXk7rMzeqG7x1tZmZWELeEzczMCuIgbGZmVhAHYTMzs4I4CJuZmRXEQdjMzKwgDsJmZmYFcRA2MzMryP8H7/n1w7mGZE8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# calling the visual_cm function\n",
    "visual_cm(true_y = y_test,\n",
    "          pred_y = tree_tuned_pred,\n",
    "          labels = ['Did Not Buy Subscription', 'Bought Subscription'],\n",
    "          title  = 'Confusion Matrix of Decision Tree Classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Observations: </strong><br>\n",
    "Our model's errors (predicted that the customer was going to buy the subscription but did not, and predicted the customer was not going to buy the subscription and did) are well balanced based on our business case. <br>\n",
    "<br>\n",
    "In other words, our marketing expenditure for <i> Halfway There </i> is being well utilized, since our precision in which customers will subscribe is 96%.\n",
    "<br> <br>\n",
    "However, our marketing expenditures could be optimized since we can see 39 out of 331 customers were not predicted to buy the subscription and still bought it. If we were to better understand these customers, we could increase the total number of customer subscriptions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Our best performing model was a Decision Tree with 21 features with a test score of 0.9058\n",
    "- Optimal features were found using exploratory data analysis, domain knowledge and tree classifiers\n",
    "- It is predicting whether or not a customer will subscribe to the new service Halfway There.\n",
    "- Its precision in correctly predicting a new customer is 96%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "675px",
    "width": "643px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "625px",
    "left": "993px",
    "right": "20px",
    "top": "73px",
    "width": "347px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
